<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> Đọc và phân tích Paper &#34;YOLOv4: Optimal Speed and Accuracy of Object Detection&#34; (YOLOv4) - HungVM2&#39;s Notes </title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="referrer" content="no-referrer">
    <meta name="description" content="" />
    <meta property="og:site_name" content="HungVM2&#39;s Notes" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hungvm2.github.io/blog/grasping-yolov4-content/" />
    <meta property="og:title" content="Đọc và phân tích Paper &#34;YOLOv4: Optimal Speed and Accuracy of Object Detection&#34; (YOLOv4)" />
    <meta property="og:image" content="https://hungvm2.github.io" />
    <meta property="og:description" content="" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:title" content="Đọc và phân tích Paper &#34;YOLOv4: Optimal Speed and Accuracy of Object Detection&#34; (YOLOv4)" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://hungvm2.github.io" />

    <link rel="canonical" href="https://hungvm2.github.io/blog/grasping-yolov4-content/">

    <link rel="stylesheet" href="https://hungvm2.github.io/css/bootstrap.min.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/github-gist.min.css" />
    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/custom.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/new-custom.css" />



    
    <link href="https://hungvm2.github.io/index.xml" rel="alternate" type="application/rss+xml" title="HungVM2&#39;s Notes" />
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {
          tags: 'all',
        inlineMath: [['$', '$']]
        }
      };
    </script>


<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    
<div class="mt-xl header">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-auto">
                <a href="https://hungvm2.github.io">
                    <h1 class="name">HungVM2&#39;s notes</h1>
                </a>
            </div>
        </div>

        <div class="row justify-content-center">
            <ul class="nav nav-primary">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/">
                        
                        Articles
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/about">
                        
                        About
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

<div class="content">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-sm-12 col-lg-8">
                <h1 class="mx-0 mx-md-4 blog-post-title">Đọc và phân tích Paper &#34;YOLOv4: Optimal Speed and Accuracy of Object Detection&#34; (YOLOv4)</h1>

                <div class="mb-md-4 meta">
                    
                    
                    <span class="author" title="Vũ Minh Hưng">
                        Vũ Minh Hưng
                    </span>
                    
                    

                    <span class="date middot" title='Sun Aug 15 2021 13:00:49 &#43;07'>
                        2021-08-15
                    </span>

                    <span class="reading-time middot">
                        11 min read
                    </span>

                    <div class="d-none d-md-inline tags">
                        <ul class="list-unstyled d-inline">
                            
                        </ul>
                    </div>
                </div>

                <div class="markdown blog-post-content">
                    
    <p>Tiếp theo trong series YOLO, ở bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper &ldquo;<strong>YOLOv4: Optimal Speed and Accuracy of Object Detection</strong>&rdquo; published tháng 4 năm 2020 nhé!</p>
<p>Đây cũng chính là version YOLO đầu tiên mà không còn được phát triển bởi người tạo ra nó: <strong>Joseph Redmon</strong>. Các tác giả từ version này, nổi bật nhất là <strong>Alexey Bochkovskiy</strong>.</p>
<p>Vì mục đích của series này là muốn đi sâu hơn vào sự tiến hoá của YOLO qua từng version, nên mình sẽ chỉ <strong>tập trung vào kiến trúc</strong> của mô hình, cũng như các <strong>phương pháp tối ưu hiệu năng và độ chính xác (FPS/mAP) mới</strong> được sử dụng so với version trước.</p>
<hr>
<h3 id="i-đọc-paper">I. Đọc Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Đọc Paper&rdquo; này, mình sẽ tóm lược lại những ý chính của tác giả, có bỏ qua một số nội dung không quá quan trọng&hellip; Khi đọc sẽ có các reference (có dạng [<a style="color:red;">?</a>]). Bạn có thể click vào các dấu hỏi đó để dẫn xuống phần &ldquo;Hiểu Paper&rdquo; bên dưới để đọc sâu hơn về vấn đề được đề cập.</p>
</blockquote>
<h4 id="1-giới-thiệu-chung-về-model">1. Giới thiệu chung về model</h4>
<p>Trong version này, các tác giả đã sử dụng nhiều những kỹ thuật khác nhau để giúp cho mạng detection đạt được độ chính xác state-of-the-art: 43.5% AP (65.7%) trên tập MS COCO và tốc độ: ~65FPS sử dụng Tesla V100.</p>
<p>YOLOV4 nhanh hơn 2 lần so với EfficientDet khi xét ở độ chính xác tương đương. So với YOLOv3, AP và FPS đều cải thiện tương ứng là 10% và 12%.</p>
<hr>
<h4 id="2-các-nghiên-cứu-được-tác-giả-sử-dụng-để-tạo-ra-yolov4">2. Các nghiên cứu được tác giả sử dụng để tạo ra YOLOv4:</h4>
<h5 id="21-tổng-quát-hoá-các-mô-hình-object-detection">2.1. Tổng quát hoá các mô hình object detection:</h5>
<p>Tác giả đã tổng hợp lại các mô hình object detection từ trước tới nay và tổng quát hoá lại thành một kiến trúc chung nhất cho các mô hình.</p>
<p>Về cơ bản, một mô hình object detection sẽ gồm có 3 phần chính: Backbone, Neck và Head.</p>
<p>Backbone chính là phần kiến trúc mạng được lấy ra từ một mạng classifier training trên tập ImageNet.</p>
<p>Head chính là phần detector, giúp xác định ra bounding box, class probailities và objectness score của các object. Gồm 2 loại, one-stage detector và two-stage detector.</p>
<p>Neck chính là các layer nằm giữa Backbone và Head, giúp thu thập các đặc trưng từ các vị trí khác nhau trên Backbone.</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/architecture.png" alt="Architecture" class="figure-img">
  <figcaption class="figure-caption">Ảnh 1: Kiến trúc tổng quát của các object detection model</figcaption>
</figure>
</div>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/object-detection-models.png" alt="Object detection" class="figure-img">
  <figcaption class="figure-caption">Ảnh 2: Các model tương ứng với mỗi thành phần của object detection</figcaption>
</figure>
</div>
<p>Ví dụ trong YOLOv3:</p>
<ul>
<li>Backbone chính là mạng Darknet-53</li>
<li>Neck chính là các layer Concatenate (các [route] layer trong file YOLOv3.cfg) để cho ra 3 output ở 3 scale khác nhau</li>
<li>Head chính là 3 output ở 3 scale cùng với bước transform output</li>
</ul>
<p>Bài toán object detection có thể chia về 2 dạng chính: Anchor based và Anchor free</p>
<p>Phần Head của detector chia ra 2 loại: one-stage detector và two-stage detector</p>
<hr class="half-line">
<h4 id="22-bag-of-freebies">2.2. Bag of freebies</h4>
<p>Bag of freebies là cụm từ được tác giả sử dụng để đặt cho các kỹ thuật giúp tối ưu mạng detector ở <b>bước training</b>.</p>
<p>Các kỹ thuật được chia về các nhóm chính sau:</p>
<ul>
<li>Data augmentation: photometric distortions, geometric distortions,  simulating object occlusion, using multiple images together, style transfer GAN</li>
<li>Dealing with semantic distribution bias: focal loss</li>
<li>Association between different class: label smoothing, knowledge distillation</li>
<li>Objective function of Bounding Box (BBox) regression: IoU loss, GIoU loss, DIoU loss, CIoU loss</li>
</ul>
<hr class="half-line">
<h4 id="23-bag-of-specials">2.3. Bag of specials</h4>
<p>Bag of specials là các kỹ thuật trong network hoặc hậu xử lý (post-processing), có thể giúp cải thiện đáng kể độ chính xác của hệ thống.</p>
<p>Các kỹ thuật này được chia về các nhóm chính sau:</p>
<ul>
<li>Enhance receptive field: SPP, ASPP, RFB</li>
<li>Attention mechanism: channel-wise attention có Squeeze-and-Excitation (SE), point-wise attention có Spatial Attention Module (SAM)</li>
<li>Feature integration: skip connection, hyper-column, FPN và các biến thể của nó như SFAM, ASFF, BiFPN</li>
<li>Activation function: ReLU, LReLU, PReLU, ReLU6, Scaled Exponential Linear Unit (SELU), Swish, hard-Swish, Mish</li>
<li>Post-processing: NMS, soft NMS, DIoU NMS. Bước post-processing này không cần thiết ở kiến trúc Anchor-free (ko sử dụng Anchor box)</li>
</ul>
<hr>
<h3 id="3-lựa-chọn-các-kỹ-thuật-cho-hệ-thống-detection-yolov4">3. Lựa chọn các kỹ thuật cho hệ thống detection YOLOv4</h3>
<h4 id="31-lựa-chọn-kiến-trúc-mạng">3.1. Lựa chọn kiến trúc mạng:</h4>
<p>Sau nhiều thử nghiệm và nghiên cứu các tác giả đi đến kết luận lựa chọn kiến trúc như sau:</p>
<ul>
<li>Backbone: tác giả so sánh 2 mạng CSPResNeXt-50 và CSPDarknet53, và kết luận sử dụng mạng CSPDarknet53 [<a href="#cspdarknet53" style="color:red;">?</a>]<a name="cspdarknet53_b"></a></li>
<li>Neck sử dụng SPP [<a href="#spp" style="color:red;">?</a>]<a name="spp_b" style="color:red;"></a>, PANet [<a href="#pan" style="color:red;">?</a>]<a name="pan_b" style="color:red;"></a></li>
<li>Head sử dụng YOLOv3 (anchor based)</li>
</ul>
 <hr class="half-line">
<h4 id="32-lựa-chọn-bag-of-freebies">3.2. Lựa chọn Bag of Freebies:</h4>
<ul>
<li>Cho phần Backbone: CutMix [<a href="#cutmix" style="color:red;">?</a>]<a name="cutmix_b" style="color:red;"></a>, Mosaic data augmentation [<a href="#mda" style="color:red;">?</a>]<a name="mda_b" style="color:red;"></a>, DropBlock regularization [<a href="#dbr" style="color:red;">?</a>]<a name="dbr_b" style="color:red;"></a>, Class label smoothing [<a href="#cls" style="color:red;">?</a>]<a name="cls_b" style="color:red;"></a></li>
<li>Cho mạng detector: CIoU-loss, CmBN, DropBlock regularization, Mosaic data augmentation, Self-Adversarial Training, Eliminate grid sensitivity, Using multiple anchors for a single ground truth, Cosine annealing scheduler, Optimal hyper-parameters, Random training shapes</li>
</ul>
<hr class="half-line">
<h4 id="33-lựa-chọn-bag-of-specials">3.3. Lựa chọn Bag of Specials:</h4>
<ul>
<li>Cho phần Backbone: Mish activation, Cross-stage partial connections (CSP), Multiinput weighted residual connections (MiWRC)</li>
<li>Cho mạng detector:  Mish activation, SPP-block, SAM-block, PAN path-aggregation block, DIoU-NMS</li>
</ul>
<h4 id="34-hàm-loss">3.4. Hàm Loss:</h4>
<p>Vì Yolov4 vẫn sử dụng Head detector giống như YOLOv3 nên công thức hàm Loss vẫn giữ nguyên:</p>
<hr>
<h3 id="4-kết-quả-thử-nghiệm">4. Kết quả thử nghiệm:</h3>
<h4 id="41-ảnh-hưởng-của-các-kỹ-thuật-đến-độ-chính-xác-của-mạng-classifier">4.1. Ảnh hưởng của các kỹ thuật đến độ chính xác của mạng classifier</h4>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/bof-mish-on-classifier.png" alt="BoF, Mish influences on the classifier" class="figure-img">
  <figcaption class="figure-caption">Ảnh 1: Ảnh hưởng của các kỹ thuật Bag of Freebies và hàm Mish activation đến độ chính xác của mạng classifier</figcaption>
</figure>
</div>
<hr class="half-line">
<h4 id="42-ảnh-hưởng-của-các-kỹ-thuật-đến-độ-chính-xác-của-mạng-detector">4.2. Ảnh hưởng của các kỹ thuật đến độ chính xác của mạng detector</h4>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/bof-loss-on-detector.png" alt="BoF, Loss influences on the detector" class="figure-img">
  <figcaption class="figure-caption">Ảnh 2: Ảnh hưởng của các kỹ thuật Bag of Freebies, Loss... đến độ chính xác của mạng classifier CSPDarknet-53(Size: 512x512)</figcaption>
</figure>
</div>
<p>Giải thích các ký hiệu:</p>
<ul>
<li>S: điều chỉnh phương pháp tính $b_{x}, b_{y}$ của YOLOv3</li>
<li>M: Mosaic data augmentation</li>
<li>IT: IoU threshold, sử dụng nhiều anchor cho 1 ground truth</li>
<li>GA: Genetic algorithms</li>
<li>LS: Class label smoothing</li>
<li>CBN: CmBN - sử dụng Cross mini-Batch Normalization</li>
<li>CA: Cosine annealing scheduler</li>
<li>DM: Dynamic mini-batch size</li>
<li>OA: Optimized Anchors</li>
<li>GIoU, CIoU, DIoU, MSE: các thuật toán khác nhau khi predict giá trí bounding box</li>
</ul>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/bos-on-detector.png" alt="BoF, Loss influences on the detector" class="figure-img">
  <figcaption class="figure-caption">Ảnh 3: Ảnh hưởng của các kỹ thuật Bag of Specials... đến độ chính xác của mạng detector CSPResNeXt50-PANet_SPP (tác giả không có kết quả đánh giá cho backbone CSPDarknet-53),(Size: 512x512)</figcaption>
</figure>
</div>
<hr class="half-line">
<h4 id="43-ảnh-hưởng-của-các-backbone-đến-kết-quả-mạng-detector">4.3. Ảnh hưởng của các backbone đến kết quả mạng detector:</h4>
<ul>
<li><b>Backbone có độ chính xác nhất với bài toán classification không nhất thiết đem lại độ chính xác cao nhất cho bài toán detector</b>: CSPResNeXt-50 có độ chính xác cao hơn CSPDarknet53 ở bài toán classification, nhưng lại thấp hơn trong bài toán detection.</li>
<li>Các kỹ thuật BoF và Mish giúp tăng accuracy của CSPResNeXt50 ở bài toán classfication nhưng lại giảm accuracy ở bài toán detection, CSPDarknet53 thì ngược lại.</li>
</ul>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/cspresnetxt50-cspdarknet53-comparision.png" class="img-fluid" alt="CSPResNeXt50 - CSPDarknet53 comparision">
</div>
<h4 id="43-ảnh-hưởng-của-mini-batch-size-đến-kết-quả-training-mạng-detector">4.3. Ảnh hưởng của mini-batch size đến kết quả training mạng detector:</h4>
<p>Khi sử dụng BoF và BoS, size của mini-batch không còn ảnh hưởng đến kết quả training.</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/batch-size-on-detector.png" alt="Batch size on the detector" class="figure-img">
  <figcaption class="figure-caption">Ảnh 4: Ảnh hưởng của kích thước mini-batch đến độ chính xác của mạng detector (Size: 512x512)</figcaption>
</figure>
</div>
<h4 id="44-so-sánh-độ-chính-xác-và-hiệu-năng-của-yolov4-với-các-mô-hình-object-detector-khác">4.4. So sánh độ chính xác và hiệu năng của YOLOv4 với các mô hình object detector khác:</h4>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/ap-fps-comparison.png" class="img-fluid" alt="AP, FPS comparison">
</div>
<p>Trong paper có bảng so sánh rất chi tiết giữa các object detector khác nhau, các bạn có thể tham khảo thêm ở paper.</p>
<hr>
<h3 id="ii-hiểu-paper">II. Hiểu Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Hiểu Paper&rdquo; này, mình sẽ đi vào phân tích cũng như dẫn lại các giải thích của tác giả về các lý do tại sao tác giả chọn hướng tiếp cận đó, cũng như các thuật ngữ, công thức trong paper&hellip;</p>
</blockquote>
<p>[<a href="#cspdarknet53_b" style="color:red;">$ \triangleleft $</a>]<a name="cspdarknet53"></a>:<strong>Kiến trúc CSPDarknet53?</strong></p>
<p>CSPDarknet53 chính là sự kết hợp của mạng Darknet-53 ở YOLOv3 và kỹ thuật của CSPNet.
Building block của Darknet53 là block Residual, còn của CSPDarknet53 là block Residual kết hợp với Cross Stage Partial.</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov4/building-block.png" alt="Building block" class="figure-img">
  <figcaption class="figure-caption">Building block của Darknet53</figcaption>
</figure>
</div>
<p>Gọi $CBA_{i}$ là stack thứ $i$ của 3 layer: [Conv layer =&gt; BatchNorm layer =&gt; Activation layer]. $X_{i}$ là feature map ở stack thứ $i$.
Công thức cho 1 building block sẽ được mô tả như bên dưới:</p>
<ul>
<li>Darknet53:</li>
</ul>
<div>
\begin{aligned}
&X_{i} = CBA_{i}(X_{i-1})\\
&X_{i+1} = CBA_{i+1}(X_{i})\\
&X_{i+2} = CBA_{i+2}(X_{i+1})\\
&X_{i+3} = CBA_{i+3}(X_{i+1} + X_{i})\\
&X_{i+4} = CBA_{i+4}(X_{i+3})\\
\end{aligned}
</div>
<ul>
<li>CSPDarknet53:</li>
</ul>
<div>
\begin{aligned}
&X_{i} = CBA_{i}(X_{i-1})\\
&X^{'}_{i+1} = CBA^{'}_{i+1}(X_{i})\\
&X_{i+1} = CBA_{i+1}(X_{i})\\
&X_{i+2} = CBA_{i+2}(X_{i+1})\\
&X_{i+3} = CBA_{i+3}(X_{i+1} + X_{i})\\
&X^{'}_{i+3} = concat(X_{i+3}, X^{'}_{i+1})\\
&X_{i+4} = CBA_{i+4}(X^{'}_{i+3})\\
\end{aligned}
</div>
<hr class="half-line">
<p>[<a href="#spp_b" style="color:red;">$ \triangleleft $</a>]<a name="spp"></a>:<strong>Kiến trúc của SPP?</strong>
SPP là Spatial Pyramid Pooling. Bên dưới là kiến trúc của SPP trong một mạng classifier.</p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/spp.png" class="img-fluid" alt="SPP">
</div>
<p>Do trong một mạng classifier, output layer luôn có shape cố định, vì vậy đòi hỏi các layer đều phải có kích thước cố định. Đó là lí do thường thấy các mạng classifier phải resize input layer về một kích thước cố định (VD mạng resnet phải có input luôn là 224x224).</p>
<p>SPP sẽ giúp cho input layer có thể nhận kích thước tuỳ ý, những vẫn sẽ output ra được kích thước cố định bất kể input thay đổi thế nào. Bên cạnh đó, nó cũng giúp cho mô hình trở nên mạnh mẽ hơn.</p>
<p>Tuy nhiên trong YOLOv4, vì không có các lớp Fully connected layer phía sau nên lớp SPP này chỉ giúp mô hình mạnh hơn khi trích xuất đặc trưng thôi.</p>
<p>Ý tưởng khá đơn giản, trong YOLOv4, SPP sẽ gồm có 3 Max Pooling layer, sau đó concat lại với input của SPP để tạo thành output, như đoạn code bên dưới:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#008000"># Xem chi tiết hơn tại https://github.com/Tianxiaomo/pytorch-YOLOv4/blob/master/models.py , line 284</span>
x3 = conv3(x2)
maxpool1 = MaxPool2d(kernel_size=5, stride=1, padding=5 // 2)
maxpool2 = MaxPool2d(kernel_size=9, stride=1, padding=9 // 2)
maxpool3 = MaxPool2d(kernel_size=13, stride=1, padding=13 // 2)
m1 = maxpool1(x3)
m2 = maxpool2(x3)
m3 = maxpool3(x3)
spp = concat([m3, m2, m1, x], dim=1)
</code></pre></div><hr class="half-line">
<p>[<a href="#pan_b" style="color:red;">$ \triangleleft $</a>]<a name="pan"></a>:<strong>Kiến trúc của PANet?</strong>
PANet là Path Agrregation Network. Một kiến trúc nâng cao từ Feature Pyramid Network. Kiến trúc mới này giúp mô hình nhận được chính xác hơn các tín hiệu vị trí (localization signals) của các object ở các low-level layers, giúp predict toạ độ object chính xác hơn.</p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/panet.png" class="img-fluid" alt="PANet">
</div>
<p>YOLOv4 chỉ sử dụng 2 phần FPN backbone và Bottom-up path augmentation bên trên vào kiến trúc của mình, phần shortcut (đường đứt xanh và đỏ) cũng không được sử dụng. Điều này được thể hiện rõ trong file <a href="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4-csp.cfg">config</a></p>
<hr class="half-line">
<p>[<a href="#cutmix_b" style="color:red;">$ \triangleleft $</a>]<a name="cutmix"></a>:<strong>CutMix là gì?</strong></p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/cutmix.png" class="img-fluid" alt="CutMix">
</div>
<p>CutMix là một kỹ thuật augment data, là sự kết hợp giữa 2 phương pháp Mixup và Cutout, một training data mới sẽ được tạo thành từ 2 image trong dataset.</p>
<p>Data point mới được tạo thành sẽ có chứa thông tin của 2 lớp đối tượng từ 2 images với giá trị tương ứng theo tỷ lệ diện tích mà mỗi class chiếm chỗ trong data. [<a href="#cutmixp" style="color:red;">?</a>]<a name="cutmixp_b" style="color:red;"></a></p>
<hr class="half-line">
<p>[<a href="#mda_b" style="color:red;">$ \triangleleft $</a>]<a name="mda"></a>:<strong>Mosaic Data Augmentation là gì?</strong></p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/mosaic.png" class="img-fluid" alt="Mosaic Augmentation">
</div>
<p>Phương pháp augmentation này khá đơn giản, được tác giả Yolov4 đề xuất. Mỗi training image sẽ được ghép từ 4 image khác nhau, như vậy 4 ngữ cảnh (context) của 4 bức ảnh sẽ nằm cùng trên 1 data point, điều này giúp mô hình có thể phát hiện ra vật thể nằm ngoài những context thông thường của training data.</p>
<hr class="half-line">
<p>[<a href="#dbr_b" style="color:red;">$ \triangleleft $</a>]<a name="dbr"></a>:<strong>DropBlock Regularization là gì?</strong></p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov4/dropblock.png" class="img-fluid" alt="Dropblock">
</div>
<p>Ý tưởng của DropBlock tương tự như DropOut, cũng là một phương pháp regularization (chính quy hóa). Nhưng thay vì ngẫu nhiên tắt các unit ở các vị trí khác nhau trên feature map như DropOut, DropBlock tắt cụm các unit liền kề nhau.</p>
<p>Phương pháp này giúp cho một vùng thông tin sẽ được gỡ bỏ khỏi feature map, thay vì các điểm thông tin rời rạc, giúp cho mạng tổng quát hóa tốt hơn, giảm overfit. [<a href="#dropblockp" style="color:red;">?</a>]<a name="dropblockp_b" style="color:red;"></a></p>
<hr class="half-line">
<p>[<a href="#cls_b" style="color:red;">$ \triangleleft $</a>]<a name="cls"></a>:<strong>Class label smoothing là gì?</strong></p>
<p>Là một kỹ thuật <strong>regularization</strong> sử dụng khi biển diễn giá trị nhãn (label) của data trong bài toàn Classification. [<a href="#clsp" style="color:red;">?</a>]<a name="clsp_b" style="color:red;"></a></p>
<p>Thông thường khi đánh nhãn dữ liệu, giá trị nhãn thường được biểu diễn ở dạng one-hot vector. Ví dụ như sau:</p>
<div class="my-3">
$[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]$
</div>
<p>Tuy nhiên điều này đôi lúc sẽ khiến cho model bị overfit do dự đoán quá tự tin giá trị nhãn của dữ liệu.</p>
<p>Để xử lý vấn đề này, label sẽ không được biển diễn ở dạng one-hot vector nữa mà sẽ ở <i>dạng smooth</i> như sau:</p>
<div class="my-3">
$[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.91 0.01 0.01]$
</div>
<p>Gọi $q_{i}$ là giá trị của phần tử thứ i trong vector. $y$ là vị trí giá trị của class. $\varepsilon$ là hyperparams độ smooth. Ta có công thức tổng quát cho giá trị các phần tử của vector như sau:</p>
<div class="my-3">
$q_{i} = 
        \begin{cases}
        1 - \varepsilon & \quad \text{nếu} \quad i = y\\
        \varepsilon/(K-1) & \quad \text{nếu ngược lại,}\\
        \end{cases}$
</div>
<p>$\varepsilon = 0.09, y = 7$ thì nhãn sẽ là vector có <i>dạng smooth</i> như phía trên.</p>
<hr>
<h3 id="iii-nguồn-tham-khảo">III. Nguồn tham khảo:</h3>
<ul>
<li><a href="https://arxiv.org/abs/2004.10934">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></li>
<li><a href="https://github.com/Tianxiaomo/pytorch-YOLOv4">pytorch-YOLOv4</a></li>
<li><a href="https://arxiv.org/abs/1406.4729">Spatial Pyramid Pooling</a></li>
<li><a href="https://arxiv.org/abs/1803.01534">Path Aggregation Network</a></li>
<li>[<a href="#dropblockp_b" style="color:red;">$ \triangleleft $</a>]<a name="dropblockp"></a><a href="https://arxiv.org/pdf/1810.12890.pdf">DropBlock: A regularization method for
convolutional networks</a></li>
<li>[<a href="#cutmixp_b" style="color:red;">$ \triangleleft $</a>]<a name="cutmixp"></a><a href="https://arxiv.org/pdf/1905.04899.pdf">CutMix: Regularization Strategy to Train Strong Classifiers
with Localizable Features</a></li>
<li>[<a href="#clsp_b" style="color:red;">$ \triangleleft $</a>]<a name="clsp"></a><a href="https://arxiv.org/pdf/1812.01187.pdf">Bag of Tricks for Image Classification with Convolutional Neural Networks</a></li>
</ul>



                </div>

                
            </div>
        </div>
    </div>
</div>





    

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stan.min.js" defer></script>
        
        <script>
            window.addEventListener('load', function() {
                hljs.initHighlighting();
            }, true);
        </script>
    

    

    



</body>
</html>
