<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1) - HungVM2&#39;s Notes </title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="referrer" content="no-referrer">
    <meta name="description" content="" />
    <meta property="og:site_name" content="HungVM2&#39;s Notes" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hungvm2.github.io/blog/grasping-yolov1-content/" />
    <meta property="og:title" content="Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)" />
    <meta property="og:image" content="https://hungvm2.github.ioimages/yolov1/grid_cell.png" />
    <meta property="og:description" content="" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:title" content="Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://hungvm2.github.ioimages/yolov1/grid_cell.png" />

    <link rel="canonical" href="https://hungvm2.github.io/blog/grasping-yolov1-content/">

    <link rel="stylesheet" href="https://hungvm2.github.io/css/bootstrap.min.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/github-gist.min.css" />
    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/custom.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/new-custom.css" />



    
    <link href="https://hungvm2.github.io/index.xml" rel="alternate" type="application/rss+xml" title="HungVM2&#39;s Notes" />
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>










    <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>

<body>
    
<div class="mt-xl header">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-auto">
                <a href="https://hungvm2.github.io">
                    <h1 class="name">HungVM2&#39;s notes</h1>
                </a>
            </div>
        </div>

        <div class="row justify-content-center">
            <ul class="nav nav-primary">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/">
                        
                        Articles
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/about">
                        
                        About
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

<div class="content">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-sm-12 col-lg-8">
                <h1 class="mx-0 mx-md-4 blog-post-title">Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)</h1>

                <div class="mb-md-4 meta">
                    
                    
                    <span class="author" title="Vũ Minh Hưng">
                        Vũ Minh Hưng
                    </span>
                    
                    

                    <span class="date middot" title='Mon Jun 21 2021 22:44:24 &#43;07'>
                        2021-06-21
                    </span>

                    <span class="reading-time middot">
                        16 min read
                    </span>

                    <div class="d-none d-md-inline tags">
                        <ul class="list-unstyled d-inline">
                            
                        </ul>
                    </div>
                </div>

                <div class="markdown blog-post-content">
                    
    <p>Trong bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper &ldquo;<strong>You Only Look One: Unified, Real-Time Object Detection</strong>&rdquo; published tháng 5 năm 2016 nhé!</p>
<hr>
<h3 id="i-đọc-paper">I. Đọc Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Đọc Paper&rdquo; này, mình sẽ tóm lược lại những ý chính của tác giả, có bỏ qua một số nội dung: Fast YOLO&hellip; Khi đọc sẽ có các reference (có dạng [<a style="color:red;">?</a>]). Bạn có thể click vào các dấu hỏi đó để dẫn xuống phần &ldquo;Hiểu Paper&rdquo; bên dưới để đọc sâu hơn về vấn đề được đề cập.</p>
</blockquote>
<h4 id="1-giới-thiệu-chung-về-mô-hình-model">1. Giới thiệu chung về mô hình (model)</h4>
<p>Các nghiên cứu trước đây xử lý bài toán Object Detection theo hướng Classification (phân loại) [<a href="#previous_model" style="color:red;">?</a>]<a name="previous_model_b"></a>, còn tác giả YOLO lại <strong>tiếp cận theo hướng Regression</strong> (hồi quy).</p>
<p>YOLO là <strong>một Neural Network đơn nhất</strong> dự đoán đồng thời cả Bounding boxes và Class probabilities từ ảnh đầu vào qua một evaluation duy nhất.</p>
<p>Kiến trúc hợp nhất này giúp cho YOLO có các ưu điểm:</p>
<ul>
<li>Có thể xử lý ảnh ở <strong>tốc độ real-time</strong> $45$FPS. So sánh với các mô hình detect khác, YOLO mắc lỗi nhiều hơn về Localization Errors (sai vị trí của vật thể) nhưng lại ít hơn các lỗi False Positive (nhận diện nhầm Background là vật thể).</li>
<li>Học được các đặc tính của objects một cách <strong>tổng quát hơn</strong> hẳn các phương pháp khác (như là DPM, R-CNN).</li>
</ul>
<p>Tuy nhiên YOLO vẫn có nhược điểm:</p>
<ul>
<li><strong>Độ chính xác thấp hơn</strong> các phương pháp state-of-the-art khác</li>
<li><strong>Khó khăn</strong> khi xác định chính xác <strong>vị trí</strong> của các vật thể (Localization), đặc biệt các <strong>vật thể cỡ nhỏ</strong>.</li>
</ul>
<hr>
<h4 id="2-kiến-trúc-của-hệ-thống-yolo">2. Kiến trúc của hệ thống YOLO</h4>
<p>Hệ thống YOLO <strong>chia</strong> ảnh đầu vào thành <strong>một lưới</strong> $S\times S$ các ô vuông (grid cell). Nếu trung tâm (center) của vật thể rơi vào cell nào, thì cell đó sẽ chịu trách nhiệm phát hiện vật thể đó, và một grid cell chỉ có thể <strong>chứa tối đa 1 object</strong>. [<a href="#grid_cell" style="color:red;">?</a>]<a name="grid_cell_b" style="color:red;"></a></p>
<div class="text-center my-3">
<img class="img-fluid" src="https://hungvm2.github.io/images/yolov1/grid_cell.png" alt="Grid Cell">
</div>
<p><strong>Mỗi Grid cell</strong> cần dự đoán $B$ <strong>bounding boxes</strong> và $N$ giá trị <strong>xác suất của các lớp</strong> (class) đối tượng:</p>
<ul>
<li>
<p>Mỗi bounding box gồm có:</p>
<ul>
<li>
<p>4 giá trị <strong>toạ độ</strong> (coordinates): $(x, y, w, h)$. Trong đó $(x,y)$ là toạ độ tâm, $(w,h)$ là chiều dài và rộng</p>
</li>
<li>
<p>1 <strong>confidence score</strong> $C$, tính theo công thức:
\begin{equation}C = Pr(Object) \times IOU_{pred}^{truth} \label{ref_c}\end{equation}</p>
  <span style="display:inline-block">
  $Pr(Object) = 
  \begin{cases}
  0 & \quad \text{nếu không có Object trong cell}\\
  1 & \quad \text{nếu có Object trong cell}
  \end{cases}$
  </span><br>
  $IOU_{pred}^{truth}$: Giá trị IOU giữa ground truth và predicted bounding box. [<a href="#iou" style="color:red;">?</a>]<a name="iou_b" style="color:red;"></a>
</li>
</ul>
</li>
<li>
<p>Giá trị xác suất của mỗi class được tính theo công thức:
\begin{equation}p(c) = Pr(Class_{c}|Object), \quad c = (1,&hellip;,N) \label{ref_pc}\end{equation}
$Pr(Class_{c}|Object)$: Là xác suất với điều kiện có Object xuất hiện trong grid cell, là xác suất của class thứ c trong N classes.</p>
</li>
</ul>
<h5 id="21-kiến-trúc-mạng">2.1. Kiến trúc mạng:</h5>
<p>Mạng có lấy cảm hứng từ mạng Inception của Google. [<a href="#1by1_conv" style="color:red;">?</a>]<a name="1by1_conv_b"></a></p>
<p>Mạng gồm 24 Convolutional (Conv) Layers để trích xuất đặc trưng (extract features) và 2 Fully Connected (FC) Layers để dự đoán final output [<a href="#network_detail" style="color:red;">?</a>]<a name="network_detail_b"></a>.</p>
<div class="text-center my-4">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/network_architecture.png" alt="Network Architecture" class="figure-img">
  <figcaption class="figure-caption"><a name="anh_1_b"></a>Ảnh 1: Kiến trúc mạng detection của YOLO</figcaption>
</figure>
</div>
<p><em>Kích thước Output của Network:</em></p>
<p>\begin{equation}Ouput\quad Shape = (S , S , B \times 4 + B + C) = (S, S , B \times 5 + C)\end{equation}</p>
<h5 id="22-các-bước-khi-huấn-luyện-mô-hình-training">2.2. Các bước khi huấn luyện mô hình (training):</h5>
<p><strong>Bước 1</strong>: Đặt 20 lớp Conv Layers đầu tiên của mạng detection (<a href="#anh_1_b" style="color:red;">Ảnh 1</a><a name="anh_1"></a>) trong 1 kiến trúc mạng Classification [<a href="#classification_network" style="color:red;">?</a>]<a name="classification_network_b"></a> , và train trên tập ImageNet 1000 classes.</p>
<p><strong>Bước 2</strong>: Sử dụng Transfer Learning, đặt lại Weights của 20 lớp Conv Layers đã được train ở bước 1 trở lại mạng detection (Ảnh 1). Do <strong>Weights</strong> của phần <strong>Conv Layers không phụ thuộc</strong> vào <strong>Input size</strong> nên train mạng detection với Input size tăng lên $448\times 448$ để mạng học được những đặc trưng chi tiết hơn của ảnh.</p>
<p><strong>Bước 3</strong>: Lấy kết quả output của mạng detection, biến đổi lại các giá trị nằm trong khoảng $[0,1]$ [<a href="#feature_scaling" style="color:red;">?</a>]<a name="feature_scaling_b"></a> trước khi đặt vào hàm Loss</p>
<ul>
<li><strong>Normalize</strong> (w,h) của bounding box để w,h trong đoạn [0,1]: (w = w/W , h = h/H | W,H: size của input)</li>
<li>Biến đổi toạ độ (x,y) thành toạ độ <strong>offset từ top-left của grid cell</strong> mà nó nằm trong để x, y nằm trong đoạn [0,1]. VD: x,y nằm chính giữa cell thì x = 0.5, y = 0.5</li>
<li>Confidence score <a style="color:red;">(\ref{ref_c})</a>: $C = IOU$ hoặc $C = 0$ nên cũng nằm trong đoạn [0,1]</li>
<li>$p(c)$ <a style="color:red;">(\ref{ref_pc})</a>: là xác suất detect ra vật thể class c nên cũng nằm trong đoạn [0,1]</li>
</ul>
<p><strong>Bước 4</strong>: Xây dựng hàm Loss [<a href="#sse" style="color:red;">?</a>]<a name="sse_b" style="color:red;"></a> [<a href="#loss_formular" style="color:red;">?</a>]<a name="loss_formular_b"></a></p>
<div>
\begin{multline*}
\lambda_{coord} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^{2}]\\ 
+ \lambda_{coord}\displaystyle\sum_{i=0}^{S^{2}}\displaystyle\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}[(\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}]\\
+ \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} (C_{i} - \hat{C}_{i})^{2}\\
+ \lambda_{noobj} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_{i} - \hat{C}_{i})^2\\
+ \displaystyle \sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{obj} \displaystyle \sum_{c \in classes}(p_{i}(c) - \hat{p_{i}}(c))^{2}
\end{multline*}
</div>
<p><strong>Trong đó:</strong></p>
<p><em>Gọi &ldquo;Box Predictor&rdquo; là Bounding box có IOU lớn nhất trong grid cell với ground truth box</em></p>
<ul>
<li>$$ \lambda_{coord} = 5 $$ , $\lambda_{noobj} = 0.5$ [<a href="#lambda" style="color:red;">?</a>]<a name="lambda_b"></a></li>
<li>$\mathbb{1}_{i}^{obj}$ nhận giá trị 1 nếu object xuất hiện ở cell thứ $i$ ; 0 nếu ngược lại</li>
<li>$\mathbb{1}_{ij}^{obj}$ nhận giá trị 1 nếu bounding box là &ldquo;Box Predictor&rdquo; ; 0 nếu ngược lại</li>
<li>$x_{i},y_{i}$: Toạ độ tâm (đã biến đổi) của ground truth box ở grid cell thứ $i$</li>
<li>$w_{i},h_{i}$: Kích thước rộng, dài của ground truth box ở grid cell thứ $i$</li>
<li>$\hat{x}_{i}, \hat{y}_{i}$: Toạ độ tâm (đã biến đổi) của &ldquo;Box Predictor&rdquo; ở grid cell thứ $i$</li>
<li>$\hat{w}_{i}, \hat{w}_{i}$: Kích thước rộng, dài của &ldquo;Box Predictor&rdquo; ở grid cell thứ $i$</li>
<li>$C_{i}$: IOU của ground truth box với chính nó (=1) ở Grid cell thứ $i$</li>
<li>$\hat{C}_{i}$: IOU của &ldquo;Box Predictor&rdquo; với ground truth box ở Grid cell thứ $i$</li>
<li>$p_{i}(c)$: Giá trị xác suất của class thứ c (trong N classes) của ground truth data ở Grid cell thứ $i$ ($=1$ nếu ground truth box chỉ class c, $=0$ nếu ngược lại)</li>
<li>$\hat{p}_{i}(c)$: Giá trị xác suất của class thứ c (trong N classes) của predicted data ở Grid cell thứ $i$</li>
</ul>
<p><strong>Bước 5</strong>: Cấu hình và training</p>
<ul>
<li>Dataset: <strong>PASCAL VOC 2017 và 2012</strong></li>
<li>Batch size: 64 (nếu có $m$ data points trong tập train thì mỗi epoch sẽ có $m/64$ iterations)</li>
<li>Các kỹ thuật tuning được sử dụng: <em>Momentum, Decay, Learning rate schedule, Data augmentation</em>. [<a href="#tuning_technique" style="color:red;">?</a>]<a name="tuning_technique_b" style="color:red;"></a></li>
</ul>
<h5 id="23-inference-testing">2.3. Inference (Testing)</h5>
<p>Thường tâm của object sẽ chỉ nằm trong 1 gird cell, tuy nhiên có những trường hợp như object lớn hoặc tâm của object nằm gần viền của nhiều cell nên dễ bị detect ra nhiều box cho cùng một object.
Để xử lý vấn đề này, khi testing, output của Network được đi qua một bước post processing sử dụng <strong>Non-max suppression</strong> (NMS) giúp triệt tiêu những box bị trùng nhau. Bước này giúp cải thiện thêm 2-3% mAP cho model. [<a href="#nms" style="color:red;">?</a>]<a name="nms_b"></a></p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov1/nms.png" class="img-fluid" alt="NMS">
</div>
<p>Trên hình có thể thấy rõ, có 3 bounding box thuộc về 3 grid cell (3,1), (2,3), (3,3) cùng detect ra Object người. Bước NMS sẽ lấy Bounding box có Confidence Score lớn nhất trong 3 box này.</p>
<p><strong><em>Pipe line của hệ thống YOLO:</em></strong></p>
<p>Image $(448,448,3) \Rightarrow $ Network Prediction $\Rightarrow $ Output (đã reshape) $(7,7,30) \Rightarrow $ Tính lại class probabilities $Pr(c) \Rightarrow $ Bỏ qua các box có low probabilities $\Rightarrow $ Non-max suppression $\Rightarrow $ Final prediction.</p>
<p><strong><em>Giá trị xác suất nhận diện của mỗi class trong một grid cell sẽ được tính lại theo công thức sau:</em></strong> [<a href="#original_pc" style="color:red;">?</a>]<a name="original_pc_b"></a></p>
   <p style="display: inline-block;">\begin{multline*}
    Pr(c) = p(c) * C\\
     = Pr(Class_c|Object) * Pr(Object) * IOU_{pred}^{truth}\\
      = Pr(Class_{c}) * IOU_{pred}^{truth}
    \end{multline*}</p>
<hr>
<h4 id="3-so-sánh-độ-chính-xác-và-hiệu-năng-với-các-mô-hình-khác">3. So sánh độ chính xác và hiệu năng với các mô hình khác</h4>
<ul>
<li>So sánh tốc độ/mAP của YOLO, biến thể Fast YOLO với các mô hình khác</li>
</ul>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/fps_map_comparison.png" alt="FPS/mAP Comparison" class="figure-img">
  <figcaption class="figure-caption">Ảnh 3: So sánh trên tập PASCAL VOC 2007</figcaption>
</figure>
</div>
<ul>
<li>Phân tích các lỗi của YOLO so với Fast R-CNN trên tập VOC 2007</li>
</ul>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/error_analysis.png" alt="Error Analysis" class="figure-img">
  <figcaption class="figure-caption">Ảnh 4: So sánh lỗi của YOLO và Fast R-CNN</figcaption>
</figure>
</div>
<hr>
<h3 id="ii-hiểu-paper">II. Hiểu Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Hiểu Paper&rdquo; này, mình sẽ đi vào phân tích cũng như dẫn lại các giải thích của tác giả về các lý do tại sao tác giả chọn hướng tiếp cận đó, cũng như các thuật ngữ, công thức trong paper&hellip;</p>
</blockquote>
<hr class="half-line">
<p>[<a href="#previous_model_b" style="color:red;">$ \triangleleft $</a>]<a name="previous_model"></a>: <strong>Kỹ thuật sử dụng ở một số phương pháp trước đây?</strong></p>
<ul>
<li>Deformable Parts Models (DPM) sử dụng &ldquo;<strong>Sliding Window</strong>&rdquo; sau đó phân loại trên các &ldquo;Window&rdquo; này</li>
<li>R-CNN sử dụng &ldquo;<strong>Region Proposal</strong>&rdquo; để đề xuất ra các Bounding boxes tiềm năng, sau đó phân loại (Classify) trên những boxes này và một bước hậu xử lý được sử dụng để định vị lại chính xác các Bounding boxes, bỏ đi các boxes bị trùng lặp&hellip; Hướng tiếp cận này là một Pipeline phức tạp (gồm nhiều step/model nối tiếp nhau) nên rất chậm và khó để tối ưu.</li>
</ul>
<hr class="half-line">
<p>[<a href="#network_detail_b" style="color:red;">$ \triangleleft $</a>]<a name="network_detail"></a>: <strong>Chi tiết về kiến trúc của network?</strong></p>
<p>Bạn có thể xem trong <a href="https://github.com/thtrieu/darkflow/blob/master/cfg/v1/yolo-full.cfg">file config</a> [<a href="#cfg_v1" style="color:red;">?</a>]<a name="cfg_v1_b"></a> YOLO, thứ tự các layer trong file config chính là thứ tự các layer của YOLO Network</p>
<hr class="half-line">
<p>[<a href="#grid_cell_b" style="color:red;">$ \triangleleft $</a>]<a name="grid_cell"></a>: <strong>Chia grid cell ở bước nào?</strong></p>
<p>Không có bước chia grid cell, mà việc phân định toạ độ bounding box nằm ở grid cell nào sẽ được tính toán lúc đưa vào hàm Loss để training</p>
<hr class="half-line">
<p>[<a href="#1by1_conv_b" style="color:red;">$ \triangleleft $</a>]<a name="1by1_conv"></a>: <strong>Kiến trúc mạng được truyền cảm hứng bởi mạng GoogLeNet (Inception)?</strong></p>
<p>Kiến trúc mạng của YOLO có sử dụng <strong>1x1 Conv Layer</strong>, lấy cảm hứng từ mạng Inception. 1x1 Conv layer được sử dụng để <strong>giảm</strong> (hoặc tăng) số lượng feature maps ở một layer trong khi vẫn giữ nguyên kích thước của feature maps.</p>
<p>Thường 1x1 Conv layer sẽ sử dụng trước khi qua các Conv layer 3x3, 5x5&hellip; để giúp Network giảm khối lượng tính toán.</p>
<p><strong><em>Ví dụ:</em></strong></p>
<p>Kích thước Input Tensor A (Batch size, Channel, High, Width): $(1, 20, 64, 64)$.</p>
<p>Cho Tensor A đi qua một Conv layer 3x3; Số Filters: 32; Stride = 1; Padding = 0 (Lưu ý: số channel của 1 filter luôn bằng số channel của input tensor, tuy nhiên khi ký hiệu thì quy ước ko ghi số channel của filter)</p>
<p>$\Rightarrow $ Kích thước Output sẽ là: $(1, 32, 62, 62) $, Số lượng Weights $= 32 \times (20 \times 3 \times 3) =  5760$</p>
<p>Nếu cho Tensor A đi qua 1x1 Conv Layer có 8 filters rồi qua 3x3 Conv Layer có 32 filter giống như trên (Stride = 1, Padding = 0 ở cả 2 Conv layer)</p>
<p>$\Rightarrow $ Kích thước Output vẫn là: $(1, 32, 62, 62) $, Số lượng Weights $= 8 \times (20 \times 1 \times 1) + 32 \times (8 \times 3 \times 3) = 2464$</p>
<p>Ta có thể thấy số lượng Weights đã giảm gần 1/2 cho tình huống trên, dù không thay đổi shape của Input và Output.</p>
<hr class="half-line">
<p>[<a href="#loss_formular_b" style="color:red;">$ \triangleleft $</a>]<a name="loss_formular"></a>: <strong>Hàm Loss dùng cho pretrained model của tác giả?</strong></p>
<p>Để đánh giá YOLO trên tập PASCAL POC, $S = 7, B = 2 [<a href="#why_b" style="color:red;">?</a>]<a name="why_b"></a>, C = 20$, hàm Loss vì thế sẽ có dạng:</p>
<div>
\begin{multline}
\lambda_{coord} \displaystyle \sum_{i=0}^{49} \displaystyle \sum_{j=0}^{2} \mathbb{1}_{ij}^{obj}[(x_{i} - \hat{x}\_{i})^{2} + (y_{i} - \hat{y}_{i})^{2}]\\ 
+ \lambda_{coord}\displaystyle\sum_{i=0}^{49}\displaystyle\sum_{j=0}^{2}\mathbb{1}_{ij}^{obj}[(\sqrt{w_{i}} - \sqrt{\hat{w}\_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}]\\
+ \displaystyle \sum_{i=0}^{49} \displaystyle \sum_{j=0}^{2} \mathbb{1}\_{ij}^{obj} (C_{i} - \hat{C}_{i})^{2}\\
+ \lambda_{noobj} \displaystyle \sum_{i=0}^{49} \displaystyle \sum_{j=0}^{2} \mathbb{1}\_{ij}^{noobj} (C_{i} - \hat{C}_{i})^2\\
+ \displaystyle \sum_{i=0}^{49} \mathbb{1}\_{i}^{obj} \displaystyle \sum_{c \in classes}(p_{i}(c) - \hat{p_{i}}(c))^{2}
\end{multline}
</div>
<p>Shape của output tensor, sau khi reshape sẽ là:</p>
<p>\begin{equation}Ouput\quad Shape = (S , S , B * 5 + C) = (7 , 7 , 2 * 5 + 20) = (7 , 7 , 30)\end{equation}</p>
<hr class="half-line">
<p>[<a href="#why_b_b" style="color:red;">$ \triangleleft $</a>]<a name="why_b"></a>:<strong>sao tác giả chọn B = 2 cho mỗi grid cell?</strong></p>
<p>YOLO predict 2 bounding box cho mỗi grid cell, tuỳ vào IOU của mỗi bounding box này so với ground truth, IOU lớn hơn sẽ giúp box được chọn làm Predictor.</p>
<p>Điều này sẽ dẫn tới sự chuyên môn hoá giữa 2 bounding box. Bounding box thứ nhất sẽ chuyên được chọn để detect các object có chiều dài lớn hơn chiều rộng (person), bounding box thứ 2 sẽ chuyên được chọn để detect các object có chiều rộng lớn hơn chiều dài (car).</p>
<hr class="half-line">
<p>[<a href="#cfg_v1_b" style="color:red;">$ \triangleleft $</a>]<a name="cfg_v1"></a>:<strong>Có sự khác nhau giữa file config này và file config yolov1 trên repository Darknet gốc?</strong></p>
<p>Đúng vậy, đây chính là file config ban đầu của YOLOv1 được đề cập trong paper. Còn file <a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov1.cfg">cfg của YOLOv1 trên repository Darknet</a> là một version YOLOv1 đã được tác giả update lại nên có khác một chút so với paper</p>
<p>Ở trong version YOLOv1 mới này, lớp Fully connected layer đầu tiên đã được thay thế bằng 1 lớp <strong>Locally connected layer</strong>, số lượng Bounding box trên mỗi grid cell là B = 3, nên Output của Network này không phải là $7 \times 7 \times 30$ mà là $7 \times 7 \times 35&amp;.</p>
<p>Locally connected layer: Có thể hiểu là 1 biến thể trung gian giữa Fully connected layer và Convolutional Layer. Chi tiết hơn các bạn xem ở <a href="https://prateekvjoshi.com/2016/04/12/understanding-locally-connected-layers-in-convolutional-neural-networks/">đây</a> hoặc <a href="https://pennlio.wordpress.com/2014/04/11/fully-connected-locally-connected-and-shared-weights-layer-in-neural-networks/">đây</a> nhé.</p>
<hr class="half-line">
<p>[<a href="#feature_scaling_b" style="color:red;">$ \triangleleft $</a>]<a name="feature_scaling"></a>: <strong>Tại sao phải có bước xử lý này?</strong></p>
<p>Đây là bước <strong>Scaling Features</strong>: đưa khoảng giá trị của các feature về cùng một scale trước khi vào hàm loss để giúp mô hình hội tụ nhanh và ổn định hơn khi training.</p>
<hr class="half-line">
<p>[<a href="#sse_b" style="color:red;">$ \triangleleft $</a>]<a name="sse"></a>: <strong>Hàm loss sử dụng là dạng gì?</strong></p>
<p>Hàm được sử dụng là <strong>Sum Squared Error</strong>. Tác giả giải thích lí do sử dụng hàm này vì nó đơn gian và dễ để tối ưu.</p>
<p>Tuy nhiên hàm này không đạt được độ chính xác AP (Average Precision) như mong đợi vì:</p>
<ul>
<li>Hàm này coi các giá trị về lỗi toạ độ (localization error) và lỗi phân loại (classification error) là tương đương với nhau (có thể thấy rõ trên công thức ở Ảnh 2)</li>
<li>Những grid cell không chứa object thì các giá trị Confidence scores sẽ bằng 0, việc này làm tăng sức mạnh của gradient (overpowering the gradient) ở những cell có chứa objects, khiến model bị mất ổn định. Để giải quyết, [<a href="#lambda_b" style="color:red;">$ \triangleleft $</a>]<a name="lambda"></a> tác giả thêm các hệ số &ldquo;$λ_{coord} = 5$&rdquo; và &ldquo;$λ_{noobj} = 0.5$&rdquo; để tăng loss từ toạ độ bounding box và giảm loss từ Confidence score.</li>
</ul>
<p>Khi training, mỗi grid cell cần predict B bounding boxes, tuy nhiên sẽ chỉ Bounding box có <strong>IOU lớn nhất</strong> so với ground truth box mới được chọn lựa để tính toán hàm loss (chọn làm &ldquo;<strong>predictor</strong>&quot;). Điều này sẽ giúp mỗi predicted Bounding box sẽ chỉ chuyên môn hoá vào việc detect object có kích thước, tỉ lệ nhất định, giúp tăng Recall của model</p>
<hr class="half-line">
<p>[<a href="#quare_root_wh_b" style="color:red;">$ \triangleleft $</a>]<a name="square_root_wh"></a>: <strong>Tại sao lại sử dụng căn bậc hai của (w,h)</strong></p>
<p>Lí do là vì, Box càng lớn thì sai số tuyệt đối về kích thước (w, h) giữa predicted value và ground truth value cũng sẽ càng lớn hơn là box nhỏ. Để xử lý vấn đề này, tác giả sử dụng sai số giữa căn bậc hai của kích thước predict và ground truth thay vì giá trị w,h gốc.</p>
<p><strong><em>Ví dụ:</em></strong></p>
<p><em>Nếu tính theo kích thước gốc:</em></p>
<p>Box Lớn: $W_{predict} = 9, W_{truth} = 8 \Rightarrow |W_{predict} - W_{truth}| = 1$</p>
<p>Box Nhỏ: $W_{predict} = 3, W_{truth} = 2 \Rightarrow |W_{predict} - W_{truth}| = 1$</p>
<p>$\Rightarrow $ Rõ ràng xét theo tỉ lệ sai số với kích thước thì Box nhỏ có sai số lớn hơn, nhưng số tuyệt đối độ lệch thì như nhau.</p>
<p><em>Nếu tính theo căn bậc hai</em></p>
<p>Box lớn: $|\sqrt{9} - \sqrt{8}| \approx 0.17$ , Box nhỏ: $|\sqrt{2} - \sqrt{1}| \approx 0.41$</p>
<p>$\Rightarrow $ Rõ ràng sai số box to đã bé hơn box nhỏ $\Rightarrow $ Sai số theo tỉ lệ đã được thể hiện rõ hơn nếu tính theo căn bậc hai</p>
<hr class="half-line">
<p>[<a href="#classification_network_b" style="color:red;">$ \triangleleft $</a>]<a name="classification_network"></a>: <strong>Tại sao lại cần training trước các lớp Conv Layer?</strong></p>
<p>Bước này giúp 24 lớp Conv Layers có bộ weights tốt trong việc trích xuất đặc trưng từ ảnh.</p>
<p><em>Network của mạng classification này có dạng:</em></p>
<p>Input $224 \times 224 \Rightarrow 24$ Conv layers $\Rightarrow 1$ Average Pooling layer $\Rightarrow 1$ Fully Connected Layer $\Rightarrow $ Output class.</p>
<hr class="half-line">
<p>[<a href="#tuning_technique_b" style="color:red;">$ \triangleleft $</a>]<a name="tuning_technique"></a>: <strong>Chi tiết các kỹ thuật tuning được sử dụng trong Paper?</strong></p>
<ul>
<li><em>Momentum</em>: Lấy bằng 0.9. Kỹ thuật này là một biến thể của Gradient Descent giúp model hội tụ nhanh hơn</li>
<li><em>Decay</em>: 0.0005. Ý của tác giả ở đây có lẽ là Weight decay, một kỹ thuật regularization giúp giảm High Variance (Overfitting)</li>
<li><em>Learning rate (LR) schedule</em>: tăng dần LR từ $10^{-3}$ tới $10^{-2}$ thay vì để LR cao từ đầu để tránh model bị phân kỳ (diverge) giai đoạn đầu, sau đó giữ nguyên LR $10^{-2}$ trong 75 epochs, giảm xuống $10^{-3}$ cho 30 epochs tiếp theo, giảm xuống $10^{-4}$ cho 30 epochs cuối.</li>
<li><em>Data augmentation</em> (làm giàu bộ dữ liệu): Random scaling (phóng to, thu nhỏ ngẫu nhiên), random translation (xê dịch vị trí pixel ngẫu nhiên) tới 20% kích thước ảnh. Ngẫu nhiên điều chỉnh độ phơi sáng (exposure) và bão hoà (saturation) của ảnh tới 1.5 lần trong không gian màu HSV. Data augmentation được sử dụng để giúp giảm overfit cho model</li>
</ul>
<hr class="half-line">
<p>[<a href="#original_pc_b" style="color:red;">$ \triangleleft $</a>]<a name="original_pc"></a>: <strong>Công thức này khác với công thức gốc trong paper?</strong></p>
<p>Mình chỉ thay đổi một chút về ký hiệu để đỡ lầm tưởng ký tự $i$ có liên quan đến số thứ tự của Grid cell trong công thức hàm Loss. Bên dưới là công thức gốc:</p>
<div class="text-center my-3">
  <img src="https://hungvm2.github.io/images/yolov1/final_class_probs.png" alt="Class Probabilities" class="figure-img">
</div>
<hr class="half-line">
<p>[<a href="#iou_b" style="color:red;">$ \triangleleft $</a>]<a name="iou"></a>: <strong>IOU được tính như thế nào?</strong></p>
<p><em>Cách tính IOU được minh hoạ bằng hình vẽ sau:</em></p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov1/IOU.png" class="img-fluid" alt="IOU">
</div>
<p><em>Hoặc bằng code Python minh hoạ đơn giản như bên dưới:</em></p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00f">def</span> iou(box1, box2):
    <span style="color:#008000"># box1, box2 là mảng chứa ít nhất 4 phần tử: x,y,w,h</span>
    <span style="color:#008000"># x,y: toạ độ tâm; w,h: kích thước rộng, dài của box</span>
    x1, y1, w1, h1 = box1[:4]
    x2, y2, w2, h2 = box2[:4]
    <span style="color:#008000"># Tính toán (left, top) và (right, bot) </span>
    <span style="color:#008000"># của intersection (vùng giao nhau giữa 2 box) </span>
    i_left, i_top = max(x1 - int(w1/2), x2 - int(w2/2)), \
                    max(y1 - int(h1/2), y2 - int(h1/2))
    i_right, i_bot = min(x1 + int(w1/2), x2 + int(w2/3)), \
                    min(y1 + int(h1/2), y2 + int(h1/2))
    intersection_area = (i_right - i_left) * (i_bot - i_top)
    union_area = (w1 * h1 + w2 * h2 - intersection_area)
    <span style="color:#00f">return</span> intersection_area/union_area
</code></pre></div><hr class="half-line">
<p>[<a href="#nms_b" style="color:red;">$ \triangleleft $</a>]<a name="nms"></a>:<strong>NMS được tính như thế nào?</strong></p>
<p><em>Thuật toán NMS gồm các bước sau:</em></p>
<ul>
<li>Input: Mảng P có chứa các Bounding box từ output của Network, mỗi bounding box đã được xử lý để có dạng [x, y, w, h, p(c), c]. Trong đố: p(c) là giá trị xác suất lớn nhất trong N classes, c là id của class có xác suất lớn nhất.</li>
<li><em>Bước 1</em>: Khởi tạo mảng F rỗng sẽ chứa các Bounding box cuối cùng, (i = 0)</li>
<li><em>Bước 2</em>: Lấy phần tử B ở vị trí (i) trong P</li>
<li><em>Bước 3</em>: So sánh B với các phần tử khác mà có chung id (c) và IOU với B lớn hơn threshold (VD: IOU &gt;=0.4)</li>
<li><em>Bước 4</em>: Nếu xác suất p(c) của B là lớn nhất thì thêm vào F</li>
<li><em>Bước 5</em>: (i++), lặp lại từ bước 2</li>
<li>Output : Mảng F có chứa các Bounding box cuối cùng</li>
</ul>
<p><em>Hoặc minh hoạ bằng đoạn code bên dưới:</em></p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00f">def</span> iou(box1, box2):
    <span style="color:#008000"># cách tính iou xem bên trên</span>
    <span style="color:#00f">pass</span>

<span style="color:#00f">def</span> nms(boxes_list, iou_threshold):
    <span style="color:#008000"># Mỗi box trong boxes_list có dạng: [x, y, w, h, p(c), c]</span>
    final_list = list()
    <span style="color:#00f">for</span> i <span style="color:#00f">in</span> range(len(boxes_list)): 
        box_i = boxes_list[i]
        discard = False
        <span style="color:#00f">for</span> j <span style="color:#00f">in</span> range(len(boxes_list)): 
            <span style="color:#00f">if</span> i == j: <span style="color:#00f">continue</span>
            box_j = boxes_list[j]
            <span style="color:#00f">if</span> box_i[5] == box_j[5] <span style="color:#00f">and</span> iou(box_i, box_j) &gt; iou_threshold \
                    <span style="color:#00f">and</span> box_i[4] &lt; box_j[4]:
                discard = True
                <span style="color:#00f">break</span>
        <span style="color:#00f">if</span> <span style="color:#00f">not</span> discard: final_list.append(box_i)
    <span style="color:#00f">return</span> final_list 
</code></pre></div><hr>
<h3 id="iii-nguồn-tham-khảo">III. Nguồn tham khảo:</h3>
<ul>
<li><a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a></li>
<li><a href="https://github.com/pjreddie/darknet">Darknet</a></li>
<li><a href="https://stats.stackexchange.com/questions/194142/what-does-1x1-convolution-mean-in-a-neural-network">What does 1x1 Convolution mean in a Neural Network</a></li>
<li><a href="https://github.com/thtrieu/darkflow">Darkflow</a></li>
</ul>



                </div>

                
            </div>
        </div>
    </div>
</div>





    

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stan.min.js" defer></script>
        
        <script>
            window.addEventListener('load', function() {
                hljs.initHighlighting();
            }, true);
        </script>
    

    

    
    
        
<script src="https://hungvm2.github.io/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
</body>
</html>
