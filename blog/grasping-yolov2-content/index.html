<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34; (YOLOv2) - HungVM2&#39;s Notes </title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="referrer" content="no-referrer">
    <meta name="description" content="" />
    <meta property="og:site_name" content="HungVM2&#39;s Notes" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hungvm2.github.io/blog/grasping-yolov2-content/" />
    <meta property="og:title" content="Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34; (YOLOv2)" />
    <meta property="og:image" content="https://hungvm2.github.io" />
    <meta property="og:description" content="" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:title" content="Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34; (YOLOv2)" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://hungvm2.github.io" />

    <link rel="canonical" href="https://hungvm2.github.io/blog/grasping-yolov2-content/">

    <link rel="stylesheet" href="https://hungvm2.github.io/css/bootstrap.min.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/github-gist.min.css" />
    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/custom.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/new-custom.css" />



    
    <link href="https://hungvm2.github.io/index.xml" rel="alternate" type="application/rss+xml" title="HungVM2&#39;s Notes" />
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {
          tags: 'all',
        inlineMath: [['$', '$']]
        }
      };
    </script>


<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    
<div class="mt-xl header">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-auto">
                <a href="https://hungvm2.github.io">
                    <h1 class="name">HungVM2&#39;s notes</h1>
                </a>
            </div>
        </div>

        <div class="row justify-content-center">
            <ul class="nav nav-primary">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/">
                        
                        Articles
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/about">
                        
                        About
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

<div class="content">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-sm-12 col-lg-8">
                <h1 class="mx-0 mx-md-4 blog-post-title">Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34; (YOLOv2)</h1>

                <div class="mb-md-4 meta">
                    
                    
                    <span class="author" title="Vũ Minh Hưng">
                        Vũ Minh Hưng
                    </span>
                    
                    

                    <span class="date middot" title='Wed Jun 30 2021 22:28:49 &#43;07'>
                        2021-06-30
                    </span>

                    <span class="reading-time middot">
                        15 min read
                    </span>

                    <div class="d-none d-md-inline tags">
                        <ul class="list-unstyled d-inline">
                            
                        </ul>
                    </div>
                </div>

                <div class="markdown blog-post-content">
                    
    <p>Tiếp theo trong series YOLO, ở bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper &ldquo;<strong>YOLO9000: Better, Faster, Stronger</strong>&rdquo; published tháng 12 năm 2016 nhé!</p>
<p>Vì mục đích của series này là muốn đi sâu hơn vào sự tiến hoá của YOLO qua từng version, nên mình sẽ chỉ <strong>tập trung vào kiến trúc</strong> của mô hình, cũng như các <strong>phương pháp tối ưu hiệu năng và độ chính xác (FPS/mAP) mới</strong> được sử dụng so với version trước.</p>
<hr>
<h3 id="i-đọc-paper">I. Đọc Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Đọc Paper&rdquo; này, mình sẽ tóm lược lại những ý chính của tác giả, có bỏ qua một số nội dung không quá quan trọng&hellip; Khi đọc sẽ có các reference (có dạng [<a style="color:red;">?</a>]). Bạn có thể click vào các dấu hỏi đó để dẫn xuống phần &ldquo;Hiểu Paper&rdquo; bên dưới để đọc sâu hơn về vấn đề được đề cập.</p>
<p>Trong paper có một số phần không được tác giả đề cập tới (ví dụ: hàm loss, số grid cell, Reorg layer&hellip;), chỉ có ở trong source code, mình tự đưa vào để dễ nắm bắt hơn nội dung của paper</p>
</blockquote>
<h4 id="1-giới-thiệu-chung-về-model">1. Giới thiệu chung về model</h4>
<p>Tác giả giới thiệu hệ thống detect objection mới, tên là: <strong>YOLO9000</strong>. So với hệ thống trước, hệ thống này có các điểm mới:</p>
<ul>
<li>Phần detection của hệ thống có tên là <strong>YOLOv2</strong> (Better và Faster)
<ul>
<li>Vẫn xử lý được real-time</li>
<li>Xử lý được <strong>input với kích thước khác nhau</strong></li>
<li><strong>Trade-off giữa độ chính xác và hiệu năng</strong>: Đạt 76.8 mAP/67 FPS hoặc 78.6 mAP/40 FPS trên VOC 2007</li>
</ul>
</li>
<li>Kỹ thuật training cùng nhau trên cả dữ liệu object detection và classification, giúp hệ thống có thể detect được trên 9000 lớp đối tượng khác nhau (Stronger)</li>
</ul>
<hr>
<h4 id="2-các-cải-tiến-so-với-yolov1">2. Các cải tiến so với YOLOv1</h4>
<h5 id="21-better-and-faster">2.1 Better and Faster</h5>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/better.png" alt="Improvement in every technique" class="figure-img">
  <figcaption class="figure-caption">Ảnh 1: Sử cải thiện mAP cho từng kỹ thuật mới được sử dụng ở YOLOv2 so với YOLOv1</figcaption>
</figure>
</div>
<p>So với YOLOv1, YOLOv2 đạt độ chính xác cao hơn và nhanh hơn đáng kể nhờ sử dụng những kỹ thuật mới sau:</p>
<ul>
<li><em>Batch Normalization</em>: Tác giả thêm layer Batch Normalization vào tất cả các lớp Conv Layer trong Network. Kỹ thuật này giúp regularize model nên không sử dụng Drop out layer như ở YOLOv1 nữa, giúp model cải thiện 2% mAP. [<a href="#batch_normalization" style="color:red;">?</a>]<a name="batch_normalization_b"></a></li>
<li><em>High Resolution Classifier</em>: giống như YOLOv1, YOLOv2 cũng train trước các lớp Conv layer trong một mạng classification, tuy nhiên sử dụng input size $448 \times 448$ thay vì $224 \times 224$ như YOLOv1. Việc train trước các lớp Conv Layer với input độ phân giải cao đã giúp cho Network trích xuất đặc trưng tốt hơn, cải thiện thêm 4% mAP.</li>
<li>Sử dụng <em>Anchor Boxes</em>: Thay vì chỉ quy định số lượng Bounding box và để model tự predict hoàn toàn coordinates của các Bounding boxes như ở YOLOv1, YOLOv2 predict toạ độ tâm trực tiếp từ model, còn kích thước w,h của bounding box được predict theo offset của nó so với một tập 5 Anchor Boxes với các kích thước cố định cho trước. Kỹ thuật này giúp model cải thiện 7% recall. [<a href="#anchor_box" style="color:red;">?</a>]<a name="anchor_box_b"></a></li>
<li><em>Dimension Clusters</em>: Thay vì tự chọn kích thước các Anchor boxes, tác giả sử dụng K-means clustering trên tập dữ liệu training bounding box để tự động tìm ra các kích thước và tỉ lệ phù hợp nhất. [<a href="#k_means" style="color:red;">?</a>]<a name="k_means_b"></a></li>
<li><em>Direct location prediction</em>: predict toạ độ tâm của bounding box gắn với vị trí của grid cell tương ứng. Kỹ thuật này giúp cải thiện 5% độ chính xác</li>
<li><em>Fine-grained Features</em>/ <em>passthrough layer</em>: sử dụng 1 layer tên là Reorg để xử lý feature maps có độ phân giải cao hơn ở layer trước đó rồi concatenate với feature maps của layer cuối cùng để tạo thành output layer. Kỹ thuật này giúp cải thiện 1% độ chính xác</li>
<li><em>Multi-Scale training</em>: Do kiến trúc mới của Network không sử dụng FC layers nên có thể thay đổi input size tuỳ ý. Trong khi training, tác giả thay đổi input size với kích thước ngẫu nhiên sau mỗi 10 batches. Kỹ thuật này giúp Network có thể predict tốt với input ở những kích thước, độ phân giải khác nhau.</li>
</ul>
<hr class="half-line">
<h5 id="22-stronger">2.2. Stronger</h5>
<p>Các kỹ thuật được sử dụng giúp hệ thống có thể detect được hơn 9000 lớp đối tượng:</p>
<ul>
<li>Hierarchical classification</li>
<li>Dataset combination with WordTree</li>
<li>Joint classification and detection</li>
</ul>
<p>Vì đây không phải là các kỹ thuật giúp tối ưu FPS và mAP của hệ thống nên mình xin phép không đi vào chi tiết các kỹ thuật được tác giả giới thiệu ở phần này.</p>
<hr>
<h4 id="3-training">3. Training:</h4>
<h5 id="31-train-trước-các-lớp-conv-layer-trong-mạng-classifier-với-tên-gọi-darknet-19">3.1. Train trước các lớp Conv layer trong mạng classifier với tên gọi Darknet-19:</h5>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/classification_network.png" alt="Darknet-19" class="figure-img">
  <figcaption class="figure-caption">Kiến trúc mạng Darknet-19</figcaption>
</figure>
</div>
<p>$1 \times 1$ filters để compress feature presentation giữa các $3 \times 3$ Conv layers.</p>
<p><em>Global Average Pooling</em>: Layer này được thêm vào giúp loại bỏ các lớp FC layers nên loại bỏ được đáng kể parameters, giảm độ phức tạp của Network, giúp tăng tốc độ xử lý [<a href="global_average_pooling#" style="color:red;">?</a>]<a name="global_average_pooling_b"></a></p>
<p>Sử dụng Dataset Imagenet 1000 class</p>
<p>Quá trình training gồm 2 bước:</p>
<p><strong><em>Bước 1:</em></strong> traing trước 160 epoch với input size $224 \times 224$</p>
<ul>
<li>Optimization algorithm: SGD</li>
<li>Learning rate schedule: polynomial rate decay</li>
<li>Regularization techniques: Weight decay, Momentum, Data augmentation</li>
</ul>
<p><strong><em>Bước 2:</em></strong> traing tiếp 10 epochs với input size $448 \times 448$</p>
<ul>
<li>Learning rate: start với rate $10^{-3}$</li>
<li>Regularization techniques: Weight decay, Momentum, Data augmentation</li>
</ul>
<hr class="half-line">
<h5 id="32-train-mạng-detection">3.2. Train mạng detection:</h5>
<h5 id="321-set-up-kiến-trúc-mạng">3.2.1. Set up kiến trúc mạng:</h5>
<ul>
<li>Giữ lại toàn bộ phần Conv layers của mạng Darknet-19 bên trên, trừ layer $1 \times 1 \times 1000$ cuối cùng.</li>
<li>Thêm 3 Conv layers $3 \times 3 \times 1024$</li>
<li>Thêm passthrough layers bao gồm <strong>Route</strong> và <strong>Reorg</strong> [<a href="#reorg" style="color:red;">?</a>]<a name="reorg_b"></a></li>
<li>Thêm $1 \times 1$ Conv layer với số filter bằng kích thước output (số filters = B x (N + 5) = 5 x (20 + 5) = 125; N: Số class trong dataset).</li>
</ul>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov2/yolov2_architecture.png" class="img-fluid" alt="YOLOv2 Architecture">
</div>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/yolov2_architecture_2.png" alt="YOLOv2 Architecture" class="figure-img">
  <figcaption class="figure-caption"><a href="#network_img" style="color:red;">Ảnh 1</a><a name="network_img_b"></a>: Mô tả Kiến trúc mạng YOLOv2 với input shape (416x416x3)</figcaption>
</figure>
</div>
<p><em>(Tác giả không có hình vẽ chi tiết kiến trúc của YOLOv2 ở trong paper, tuy nhiên mình đã tìm được một researcher khác vẽ chi tiết YOLOv2 ở <a href="https://www.researchgate.net/publication/336177198_Determination_of_Vehicle_Trajectory_through_Optimization_of_Vehicle_Bounding_Boxes_Using_a_Convolutional_Neural_Network">paper</a> của họ và sửa lại shape của output layer như bên trên)</em></p>
<h5 id="322-hàm-loss">3.2.2. Hàm loss:</h5>
<p>Vẫn sử dụng hàm Sum Squared Error như ở YOLOv1, tuy nhiên class probabilities ở YOLOv2 không gắn với grid cell nữa mà gắn với B Anchor boxes. Vì vậy một grid cell có thể chứa tối đa B object khác nhau thay vì chỉ 1 object như ở YOLOv1.</p>
<div>
\begin{multline*}
Loss = (Loss_{coordinates} + Loss_{class\_prob}) + Loss_{IOU}\\
= \Big[ \lambda_{coord} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{ij} - \hat{x}_{ij})^{2} + (y_{ij} - \hat{y}_{ij})^{2}]\\ 
+ \lambda_{coord} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj}[(\sqrt{w_{ij}} - \sqrt{\hat{w}_{ij}})^{2} + (\sqrt{h_{ij}} - \sqrt{\hat{h}_{ij}})^{2}]\\
+ \lambda_{coord} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \displaystyle \sum_{c \in classes}(p_{ij}(c) - \hat{p}_{ij}(c))^{2} \Big]\\
+ \Big[ \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} (C_{ij} - \hat{C}_{ij})^{2}\\
+ \lambda_{noobj} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_{ij} - \hat{C}_{ij})^2 \Big]
\end{multline*}
</div>
<ul>
<li>
<p>Số lượng grid cell $S_{x}xS_{y}$ chính bằng kích thước của output feature map. Các bạn có thể thấy kiến trúc của YOLOv2 sẽ cho ra output feature map có kích thước w,h luôn bằng $1/32$ kích thước W,H của input.</p>
<p>Với $W \times H = (416 \times 416) \Rightarrow S_{x} = S_{y} = 416/32 = 13$;</p>
<p>Với $W \times H = (544 \times 544) \Rightarrow S_{x} = S_{y} = 544/32 = 17$&hellip; [<a href="#grid_cell" style="color:red;">?</a>]<a name="grid_cell_b"></a></p>
</li>
<li>
<p>Các giá trị đưa vào hàm loss là các giá trị của Predicted Box/Ground truth box đã được transform/normalize để nằm trong đoạn $[0;1]$. [<a href="#normalize" style="color:red;">?</a>]<a name="normalize_b"></a></p>
</li>
<li>
<p>Trong paper cũng như source code Darknet, tác giả không đề cập chi tiết về hàm Loss. Công thức trên là mình dựa vào paper và một số YOLOv2 implementation khác (<a href="https://github.com/thtrieu/darkflow">Darkflow</a>, <a href="https://github.com/longcw/yolo2-pytorch">Yolov2-pytorch</a>)</p>
</li>
</ul>
<h5 id="323-các-kỹ-thuật-khi-training">3.2.3. Các kỹ thuật khi training:</h5>
<ul>
<li>Learning rate schedule</li>
<li>Regularization techniques: Weight decay, Momentum, Data augmentation</li>
</ul>
<hr>
<h4 id="4-inference-testing">4. Inference (Testing)</h4>
<p><strong><em>Pipe line của hệ thống object detection YOLOv2:</em></strong></p>
<p>Image $(W,H,3) \Rightarrow $ Network Prediction $\Rightarrow $ Output $(W/32,H/32,125) \Rightarrow $ Tính lại coordinates box theo W,H ; tính lại class probabilities $Pr(c) \Rightarrow $ Bỏ qua các box có low probabilities $\Rightarrow $ Non-max suppression $\Rightarrow $ Final prediction.</p>
<hr>
<h4 id="4-so-sánh-độ-chính-xác-và-hiệu-năng-với-các-mô-hình-khác">4. So sánh độ chính xác và hiệu năng với các mô hình khác</h4>
<p>YOLOv2 đạt độ chính xác tương đương hoặc nhỉnh hơn so với các thuật toán khác cùng thời điểm, trong khi nhanh hơn nhiều lần</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/comparision_voc2007.png" alt="Comparision on VOC 2007" class="figure-img">
  <figcaption class="figure-caption">So sánh mAP/FPS của YOLOv2 so với các hệ thống Object detection khác trên tập VOC 2007</figcaption>
</figure>
</div>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/comparision_voc2012.png" alt="Comparision on VOC 2012" class="figure-img">
  <figcaption class="figure-caption">So sánh mAP/FPS của YOLOv2 so với các hệ thống Object detection khác trên tập VOC 2012</figcaption>
</figure>
</div>
<hr>
<h3 id="ii-hiểu-paper">II. Hiểu Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Hiểu Paper&rdquo; này, mình sẽ đi vào phân tích cũng như dẫn lại các giải thích của tác giả về các lý do tại sao tác giả chọn hướng tiếp cận đó, cũng như các thuật ngữ, công thức trong paper&hellip;</p>
</blockquote>
<hr class="half-line">
<p>[<a href="#batch_normalization_b" style="color:red;">$ \triangleleft $</a>]<a name="batch_normalization"></a>:<strong>Batch normalization là gì?</strong><br>
Mỗi layer đều có input và output, output của layer này là input của layer tiếp sau đó.</p>
<p>Mỗi khi weight (parameters) được update trong khi training, input ở mỗi layer sẽ bị ảnh hưởng bởi tất cả các params ở các layer trước đó.</p>
<p>Vì vậy một sự thay đổi nhỏ của params sẽ khiến cho input của layer thay đổi càng lớn khi layer càng sâu hơn.</p>
<p>Sự thay đổi về distribution của input được gọi là &ldquo;<strong>Covariance Shift</strong>&rdquo;</p>
<p>Nếu sự thay đổi về distribution của input diễn ra ở các hidden layer, thì nó được gọi là &ldquo;<strong>Internal Covariance Shift</strong>&rdquo;</p>
<p>Sự thay đổi của distribution khiến cho các layers phải liên tục adapt với distribution mới của input. Điều này khiến cho quá trình training bị kém ổn định, khiến cho mất nhiều thời gian hơn để model hội tụ.</p>
<p>Batch normalization là kỹ thuật giúp giảm Internal covariance shift từ đó giúp quá trình training ổn định và cho phép sử dụng learning rate lớn hơn, nên nhanh hội tụ hơn. [<a href="#bn_paper" style="color:red;">?</a>]<a name="bn_paper_b"></a></p>
<hr class="half-line">
<p>[<a href="#anchor_box_b" style="color:red;">$ \triangleleft $</a>]<a name="anchor_box"></a>:<strong>Chi tiết về cách tính offset của Bounding box so với Anchor Boxes</strong></p>
<p>Gọi $t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$ lần lượt là các giá trị toạ độ tâm, rộng, dài và confidence score được predict ra từ network ứng với mỗi predicted Bounding box trên 1 grid cell. Đây là các giá trị được regression từ network nên có thể to nhỏ, âm, dương tuỳ ý.</p>
<p>Gọi $p_{w}, p_{h}$ là kích thước chiều rộng, dài của 1 Anchor box. Đây là giá trị kích thước theo pixel của Anchor box.</p>
<p>Gọi $C_{x}, C_{y}$ là offset từ góc top-left của input image. 2 giá trị này sẽ bằng gía trị index hàng và cột (i và j) của grid cell trên image.</p>
<p>Gọi $b_{x}, b_{y}, b_{w}, b_{h}, C$ lần lượt là toạ độ tâm, kích thước rộng, dài và confidence score của 1 predicted bounding box (trong số 5 Bounding box/ 1 grid cell). Các giá trị này sẽ được tính như sau:</p>
<div>
\begin{multline}
\\
b_{x} = \sigma{(t_{x})} + C_{x}\\
b_{y} = \sigma{(t_{y})} + C_{y}\\
b_{w} = p_{w} \times e^{t_{w}}\\
b_{h} = p_{h} \times e^{t_{h}}\\
C = Pr(object) \times IOU(b,object) = \sigma{(t_{o})}\\
\label{ref_b}
\end{multline}
</div>
<p>Giá trị $b_{x}, b_{y}$ sử dụng hàm <i>sigmoid</i>. $\sigma{(t_{x})}, \sigma{(t_{y})}$ sẽ nằm trong đoạn [0;1].</p>
<p>Giá trị C là giá trị IOU nên nằm trong đoạn [0;1]</p>
<p>Giá trị $b_{w}, b_{h}$ sử dụng luỹ thừa cơ số e (số Euler). $e^{t_{w}}/e^{t_{h}}$ là giá trị tỉ lệ offset của bounding box so với anchor box. $b_{w}/b_{h}$ nằm trong đoạn $[0; W]/[0; H]$.</p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov2/bounding_box_n_anchor_box.png" class="img-fluid" alt="Bounding box coordinates">
</div>
<hr class="half-line">
<p>[<a href="#k_means_b" style="color:red;">$ \triangleleft $</a>]<a name="k_means"></a>:<strong>Anchor boxes được tìm bằng K-means như thế nào?</strong></p>
<p><strong><em>Nhắc lại 1 chút về K-means:</em></strong></p>
<p>K-means là thuật toán phân cụm (clustering). Thuật toán sẽ nhóm các điểm dữ liệu có cùng một tính chất vào thành 1 nhóm. Số lượng chọn do mình chọn trước (hyperparams)</p>
<p>Ý tưởng của thuật toán K-means:</p>
<ul>
<li>Bước 1: Có X data point, giả sử ta chọn phân làm K cụm</li>
<li>Bước 2: <strong>Chọn ngẫu nhiên K data point</strong> trong X làm các <strong>centeroid</strong> của mỗi cụm</li>
<li>Bước 3: Tính toán <strong>khoảng cách Euclid</strong> (Euclidean distance) từ <strong>mỗi data point đến các centeroid</strong> này,  nếu khoảng cách đến center của cụm $K_{i}$ ngắn nhất thì phân data point đó về cụm $K_{i}$</li>
<li>Bước 4: Sau khi phân hết các data point về các cụm, tính toán lại centeroid ở mỗi cụm bằng cách lấy <strong>trung bình (mean)</strong> vị trí của các data point trong mỗi cụm đó</li>
<li>Bước 5: Lặp lại từ bước 2, nếu vị trí các center không thay đổi nữa thì dừng</li>
</ul>
<p>Tìm hiểu sâu hơn về K-means các bạn có thể đọc ở <a href="https://machinelearningcoban.com/2017/01/01/kmeans/#tom-tat-thuat-toan">trang Machine leanring cơ bản</a></p>
<p><strong><em>Phân cụm anchor boxes:</em></strong></p>
<ul>
<li>
<p>Mỗi data point sẽ chính là các ground truth box trong các image ở các tập VOC và COCO data set.</p>
</li>
<li>
<p>Chọn ngẫu nhiên K ground truth box trong dataset làm center của mỗi cụm</p>
</li>
<li>
<p>Khoảng cách từ mỗi box trong dataset đến center sẽ không được tính bằng Euclidean distance như thuật toán K-means gốc, mà sẽ tính bằng công thức sau:
\begin{equation*}
d(box,centroid) = 1 - IOU(box, centroid)
\end{equation*}
Tác giả không lấy theo khoảng cách Euclid giữa các tâm của box vì nó sẽ không đánh giá đúng bản chất khoảng cách giữa các box (2 box lớn overlap nhau vẫn sẽ có euclid distance của tâm lớn hơn 2 box nhỏ không overlap).
Từ công thức ta có thể thấy giá trị khoảng cách $d$ sẽ nằm trong đoạn [0;1].</p>
</li>
<li>
<p>Sau khi phân hết data point về các cụm, centroid của mỗi cụm sẽ được tính bằng cách lấy trung bình (dài, rộng, tâm) của các box trong cụm đó</p>
</li>
<li>
<p>Tác giả thử với các lựa chọn K khác nhau và đi đến kết luận K = 5 sẽ là lựa chọn tối ưu nhất giữa model complexity và high recall</p>
</li>
</ul>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov2/k-anchor-boxes.png" class="img-fluid" alt="K Anchor Boxes">
</div>
<ul>
<li>Bức hình bên phải phía trên chỉ ra vị trí và kích thước centroid tương ứng của 5 cluster trên 2 tập COCO và VOC. Có thể thấy kích thước centroid của 2 dataset khá là tương đồng, nhưng COCO biến thiên về kích thước nhiều hơn là VOC.</li>
</ul>
<p>[<a href="#global_average_pooling_b" style="color:red;">$ \triangleleft $</a>]<a name="global_average_pooling"></a>:<strong>Cách tính Global Average Pooling</strong></p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov2/global_avg_pooling.png" class="img-fluid" alt="Global Average Pooling">
</div>
<p>Global average pooling cũng tương tự như Average Pooling, nhưng thay vì tính giá trị trung bình cho kích thước 1 filter (VD: 2x2) thì GAP tính giá trị trung bình cho toàn bộ feature map.</p>
<p>Việc sử dụng Global Average Pooling chính là lí do giúp cho mạng Darknet-19 không yêu cầu cố định input size của image.</p>
<hr class="half-line">
<p>[<a href="#reorg_b" style="color:red;">$ \triangleleft $</a>]<a name="reorg"></a>:<strong>Route layer, Reorg layer để làm gì và được tính như nào</strong></p>
<p>Route layer lấy ý tưởng tượng tự như skip connection của ResNet, layer này sẽ nhận đầu vào là 2 layer trong Network và làm nhiệm vụ concatenate chúng với nhau.</p>
<p>Bản chất của Reorg layer là tổ chức, sắp xếp lại các pixel liền kề nhau trên 1 feature map thành các pixel nằm trên các channel khác nhau. Chi tiết các bạn xem ở <a href="https://leimao.github.io/blog/Reorg-Layer-Explained/">đây</a></p>
<p>Reorg + Route được kết hợp với nhau để concatenate high-level features ở layer cuối với middle-level features ở layer trước đó, giúp model có thể xác định vị trí của các small object chính xác hơn.</p>
<p>Vì ở output layer (xem <a href="#network_img_b" style="color:red;">Ảnh 1</a><a name="network_img"></a>), tác giả concatenate feature maps của 2 layer có kích thước khác nhau (26x26 và 13x13) . Reorg layer với stride = 2, sẽ làm kích thước của feature map 26x26 giảm 1/2 thành 13x13, nhờ đó có thể concatenate được với feature map 13x13 còn lại để tạo thành final output.</p>
<p>[<a href="#grid_cell_b" style="color:red;">$ \triangleleft $</a>]<a name="grid_cell"></a>:<strong>Tại sao số grid cell (SxS) lại bằng kích thước output feature map (wxh)?</strong></p>
<p>Như bạn đã biết, ở YOLOv1, output của Network là 1 lớp FC layer với số nodes = (S x S x (B x 5 + C)) = (7 x 7 x (2 * 5 + 20)) = 1470.
1470 Nodes này được reshape về dạng (7, 7, 30) = (S, S, B x 5 + C)</p>
<p>Ở YOLOv2, output layer là Conv Layer nên ko cần sử dụng bước reshape nữa, vì output đã có sẵn dạng <em>(w, h, c)</em>. So sánh với output của YOLOv1, ta có thể thấy, <em>w</em> và <em>h</em> chính là <em>S</em>, còn <em>c</em> chính là <em>(B x 5 +C)</em> . Từ đây có thể rút ra 1 số hệ quả:</p>
<ul>
<li>Với mỗi input có <em>(W,H)</em> thì YOLOv2 sẽ cho ra output <em>(w,h)</em> =&gt; số lượng Grid cell <em>(SxS)</em> sẽ thay đổi phụ thuộc vào <em>(W,H)</em></li>
<li>Do kiến trúc của YOLOv2, <em>w = W/32</em>, <em>h = H/32</em> =&gt; giá trị W, H luôn phải chia hết cho 32, và W không nhất thiết phải bằng H</li>
</ul>
<p>[<a href="#normalize_b" style="color:red;">$ \triangleleft $</a>]<a name="normalize"></a>:<strong>Chi tiết cách normalize/transform giá trí trước khi đưa vào hàm Loss:</strong></p>
<p><strong><em>Gọi:</em></strong></p>
<ul>
<li>$W, H$ là kích thước của input (pixel)</li>
<li>$S_{x}, S_{y}$ là số grid cell theo phương x, y</li>
<li>$cell_{x}, cell_{y}$: kích thước của grid cell ($= W/S_{x} = H/S_{y} = 32$pixel)</li>
<li>$w_{g},h_{g}$ là kích thước ground truth box (pixel)</li>
<li>$w_{p}, h_{p}$ là kích thước predicted box (pixel)</li>
<li>$x_{g}, y_{g}$ là vị trí tâm của ground truth box (pixel)</li>
<li>$x_{p}, y_{p}$ là vị trí tâm của predicted box (pixel)</li>
<li>i: là thứ tự của grid cell trong S_{x} x S_{y} cells</li>
<li>j: là thứ tự của Anchor box trong B Anchors</li>
<li>$\hat{p}_{c}$: giá trị xác suất nhận diện của N class cho ground truth box (20 classes). Có dạng $[0,0,..1,&hellip;0,0,0]$. Vị trí của số $1$ là index của class trong N class.</li>
<li>$P_{c}$: vector N chiều, giá trị xác suất của predicted box (20 classes)</li>
<li>C: confident score, là giá trị IOU max của ground truth box với các anchor box trên 1 grid cell. (trường hợp grid cell có 2 ground truth box trở nên thì sẽ có 2 IOU max trở lên)</li>
<li>$\hat{C}$: confident score của ground truth box</li>
<li>$\hat{x}, \hat{y}, \hat{w}, \hat{h}$: là các giá trị coordinates của ground truth đã được biến đổi.</li>
<li>$x, y, w, h$: là các giá trị của predicted box đã được biến đổi. Tính theo công thức <a style="color:red;">(\ref{ref_b})</a></li>
</ul>
<p><strong><em>Các giá trị sẽ được biến đổi như bên dưới:</em></strong></p>
<div>
\begin{multline*}
\\
\hat{x} = (x_{g} \% S_{x}) / cell_{x}\\
\hat{y} = (y_{g} \%  S_{y}) / cell_{y}\\
\hat{w} = w_{g} / W\\
\hat{h} = h_{g} / H\\
\hat{p}(c) \text{có dạng} [0,0,1,....,0]\\
\hat{C} = 1\\
\\
x = \sigma{(t_{x})}\\
y = \sigma{(t_{y})}\\
w = p_{w} \times e^{t_{w}}\\
h = p_{h} \times e^{t_{h}}\\
p(c) = Softmax(P_{c})\\
C = IOU_{max}\\
\end{multline*}
</div>
<hr>
<h3 id="iii-nguồn-tham-khảo">III. Nguồn tham khảo:</h3>
<ul>
<li><a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a></li>
<li><a href="https://github.com/pjreddie/darknet">Darknet</a></li>
<li>[<a href="#bn_paper_b" style="color:red;">$ \triangleleft $</a>]<a name="bn_paper"></a>: <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization</a></li>
<li><a href="https://github.com/longcw/yolo2-pytorch">YOLOv2-pytorch</a></li>
<li><a href="https://github.com/thtrieu/darkflow">Darkflow</a></li>
<li><a href="https://machinelearningcoban.com/2017/01/01/kmeans">Machine leanring cơ bản</a></li>
</ul>



                </div>

                
            </div>
        </div>
    </div>
</div>





    

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stan.min.js" defer></script>
        
        <script>
            window.addEventListener('load', function() {
                hljs.initHighlighting();
            }, true);
        </script>
    

    

    



</body>
</html>
