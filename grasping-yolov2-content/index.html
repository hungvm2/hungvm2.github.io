<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34; - HungVM2&#39;s Notes </title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="referrer" content="no-referrer">
    <meta name="description" content="" />
    <meta property="og:site_name" content="HungVM2&#39;s Notes" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hungvm2.github.io/grasping-yolov2-content/" />
    <meta property="og:title" content="Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34;" />
    <meta property="og:image" content="https://hungvm2.github.io" />
    <meta property="og:description" content="" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:title" content="Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34;" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://hungvm2.github.io" />

    <link rel="canonical" href="https://hungvm2.github.io/grasping-yolov2-content/">

    <link rel="stylesheet" href="https://hungvm2.github.io/css/bootstrap.min.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/github-gist.min.css" />
    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/custom.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/new-custom.css" />



    
    <link href="https://hungvm2.github.io/index.xml" rel="alternate" type="application/rss+xml" title="HungVM2&#39;s Notes" />
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {
          tags: 'all',
        inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>
    
    <div class="mt-xl header">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-auto">
                <a href="https://hungvm2.github.io">
                    <h1 class="name">HungVM2&#39;s notes</h1>
                </a>
            </div>
        </div>

        <div class="row justify-content-center">
            <ul class="nav nav-primary">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/">
                        
                        Articles
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/about">
                        
                        About
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

    <div class="content">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-sm-12 col-lg-8">
                    <h1 class="mx-0 mx-md-4">Đọc và phân tích Paper &#34;YOLO9000: Better, Faster, Stronger&#34;</h1>
                    <div class="markdown">
                        
    <p>Tiếp theo trong series YOLO, ở bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper &ldquo;<strong>YOLO9000: Better, Faster, Stronger</strong>&rdquo; published tháng 12 năm 2016 nhé!</p>
<p>Vì mục đích của series này là muốn đi sâu hơn vào sự tiến hoá của YOLO qua từng version, nên mình sẽ chỉ <strong>tập trung vào kiến trúc</strong> của mô hình, cũng như các <strong>phương pháp tối ưu hiệu năng và độ chính xác (FPS/mAP) mới</strong> được sử dụng so với version trước.</p>
<hr>
<h3 id="i-đọc-paper">I. Đọc Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Đọc Paper&rdquo; này, mình sẽ tóm lược lại những ý chính của tác giả, có bỏ qua một số nội dung&hellip; Khi đọc sẽ có các reference (có dạng [<a style="color:red;">?</a>]). Bạn có thể click vào các dấu hỏi đó để dẫn xuống phần &ldquo;Hiểu Paper&rdquo; bên dưới để đọc sâu hơn về vấn đề được đề cập.</p>
</blockquote>
<h4 id="1-giới-thiệu-chung-về-model">1. Giới thiệu chung về model</h4>
<p>Tác giả giới thiệu hệ thống detect objection mới, tên là: <strong>YOLO9000</strong>. So với hệ thống trước, hệ thống này có các điểm mới:</p>
<ul>
<li>Phần detection của hệ thống có tên là <strong>YOLOv2</strong> (Better và Faster)
<ul>
<li>Vẫn xử lý được real-time</li>
<li>Xử lý được <strong>input với kích thước khác nhau</strong></li>
<li><strong>Trade-off giữa độ chính xác và hiệu năng</strong>: Đạt 76.8 mAP/67 FPS hoặc 78.6 mAP/40 FPS trên VOC 2007</li>
</ul>
</li>
<li>Kỹ thuật training cùng nhau trên cả dữ liệu object detection và classification, giúp hệ thống có thể detect được trên 9000 lớp đối tượng khác nhau (Stronger)</li>
</ul>
<hr>
<h4 id="2-các-cải-tiến-so-với-yolov1">2. Các cải tiến so với YOLOv1</h4>
<h5 id="21-better">2.1. Better</h5>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/better.png" alt="Improvement in every technique" class="figure-img">
  <figcaption class="figure-caption">Ảnh 1: Sử cải thiện mAP cho từng kỹ thuật mới được sử dụng ở YOLOv2 so với YOLOv1</figcaption>
</figure>
</div>
<p>So với YOLOv1, model detection YOLOv2 đạt độ chính xác cao hơn đáng kể nhờ sử dụng những kỹ thuật mới sau:</p>
<ul>
<li><em>Batch Normalization</em>: Tác giả thêm layer Batch Normalization vào tất cả các lớp Conv Layer trong Network và không sử dụng layer Drop out như ở YOLOv1. Kỹ thuật này giúp regularize model, giúp model cải thiện 2% mAP. [<a href="#batch_normalization" style="color:red;">?</a>]<a name="batch_normalization_b"></a></li>
<li><em>High Resolution Classifier</em>: giống như YOLOv1, YOLOv2 cũng train trước các lớp Conv layer trong một mạng classification, tuy nhiên sử dụng input size $448 \times 448$ thay vì $224 \times 224$ như YOLOv1. Việc train trước các lớp Conv Layer với input độ phân giải cao đã giúp cho Network trích xuất đặc trưng tốt hơn, cải thiện thêm 4% mAP.</li>
<li>Sử dụng <em>Anchor Boxes</em>: Thay vì chỉ quy định số lượng Bounding box và để model tự predict hoàn toàn coordinates của các Bounding boxes như ở YOLOv1, YOLOv2 chỉ predict toạ độ tâm trực tiếp từ model, còn kích thước w,h của bounding box được predict theo offset của nó so với một tập 5 Anchor Boxes với các kích thước cố định cho trước. Kỹ thuật này giúp model cải thiện 7% recall. [<a href="#anchor_box" style="color:red;">?</a>]<a name="anchor_box_b"></a></li>
<li><em>Dimension Clusters</em>: Thay vì tự chọn kích thước các Anchor boxes, tác giả sử dụng K-means clustering trên tập dữ liệu training bounding box để tự động tìm ra các kích thước và tỉ lệ phù hợp nhất. [<a href="#k_means" style="color:red;">?</a>]<a name="k_means_b"></a></li>
<li><em>Direct location prediction</em>: predict toạ độ tâm của bounding box gắn với vị trí của grid cell tương ứng. Kỹ thuật này giúp cải thiện 5% độ chính xác</li>
<li><em>Fine-grained Features</em>/ <em>passthrough layer</em>: concatenate feature map có độ phân giải cao hơn ở layer trước đó với feature map của layer cuối cùng để tạo thành output layer. Kỹ thuật này giúp cải thiện 1% độ chính xác</li>
<li><em>Multi-Scale training</em>: Do kiến trúc mới của Network không sử dụng FC layers nên có thể thay đổi input size tuỳ ý. Trong khi training, tác giả thay đổi input size với kích thước ngẫu nhiên sau mỗi 10 batches. Kỹ thuật này giúp Network có thể predict tốt với input ở những kích thước, độ phân giải khác nhau.</li>
</ul>
<hr class="half-line">
<h5 id="22-faster">2.2 Faster</h5>
<p>YOLOv2 cũng nhanh hơn nhờ cải tiến sau:</p>
<ul>
<li>Kiến trúc của Network với tên gọi là Darknet-19:
<ul>
<li>Gồm 29 Conv layers và 5 Max pooling layers</li>
<li>Sử dụng Batch normalization để ổn định quá trình training, tăng tốc hội tụ và regularize model</li>
<li><strong>Global average pooling</strong> ở bước prediction</li>
<li>$1 \times 1$ filters để compress feature presentation giữa các $3 \times 3$ Conv layers.</li>
</ul>
</li>
</ul>
<hr class="half-line">
<h5 id="23-stronger">2.3. Stronger</h5>
<p>Các kỹ thuật được sử dụng giúp hệ thống có thể detect được hơn 9000 lớp đối tượng:</p>
<ul>
<li>Hierarchical classification</li>
<li>Dataset combination with WordTree</li>
<li>Joint classification and detection</li>
</ul>
<p>Vì đây không phải là các kỹ thuật giúp tối ưu FPS và mAP của hệ thống nên mình xin phép không đi vào chi tiết các kỹ thuật được tác giả giới thiệu ở phần này.</p>
<hr>
<h4 id="3-training">3. Training:</h4>
<h5 id="31-train-trước-các-lớp-conv-layer-trong-mạng-classifier">3.1. Train trước các lớp Conv layer trong mạng classifier:</h5>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/classification_network.png" alt="Error Analysis" class="figure-img">
  <figcaption class="figure-caption">Kiến trúc mạng classifier</figcaption>
</figure>
</div>
<p>Sử dụng Dataset Imagenet 1000 class, gồm 2 bước:</p>
<p><strong><em>Bước 1:</em></strong> traing trước 160 epoch với input size $224 \times 224$</p>
<ul>
<li>Optimization algorithm: SGD</li>
<li>Learning rate schedule: polynomial rate decay</li>
<li>Regularization techniques: Weight decay, Momentum, Data augmentation</li>
</ul>
<p><strong><em>Bước 2:</em></strong> traing tiếp 10 epochs với input size $448 \times 448$</p>
<ul>
<li>Learning rate: start với rate $10^{-3}$</li>
<li>Regularization techniques: Weight decay, Momentum, Data augmentation</li>
</ul>
<hr class="half-line">
<h5 id="32-train-mạng-detection">3.2. Train mạng detection:</h5>
<h5 id="321-set-up-kiến-trúc-mạng">3.2.1. Set up kiến trúc mạng:</h5>
<ul>
<li>Giữ lại toàn bộ phần Conv layers của mạng clasisfier bên trên, trừ layer $1 \times 1 \times 1000$ cuối cùng</li>
</ul>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/conv_layers.png" alt="Conv layers" class="figure-img">
  <figcaption class="figure-caption">Các Conv layers được giữ lại từ mạng classifier để đặt vào mạng detection</figcaption>
</figure>
</div>
<ul>
<li>Thêm 3 Conv layers $3 \times 3 \times 1024$</li>
<li>Thêm $1 \times 1$ Conv layer với số filter bằng kích thước output (125 filters)</li>
<li>Thêm <em>passthrough layer</em></li>
</ul>
<h5 id="322-hàm-loss">3.2.2. Hàm loss:</h5>
<h5 id="323-các-kỹ-thuật-khi-training">3.2.3. Các kỹ thuật khi training:</h5>
<ul>
<li>Learning rate schedule</li>
<li>Regularization techniques: Weight decay, Momentum, Data augmentation</li>
</ul>
<hr>
<h4 id="4-so-sánh-độ-chính-xác-và-hiệu-năng-với-các-mô-hình-khác">4. So sánh độ chính xác và hiệu năng với các mô hình khác</h4>
<p>YOLOv2 đạt độ chính xác tương đương hoặc nhỉnh hơn so với các thuật toán khác cùng thời điểm, trong khi nhanh hơn nhiều lần</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/comparision_voc2007.png" alt="Comparision on VOC 2007" class="figure-img">
  <figcaption class="figure-caption">So sánh mAP/FPS của YOLOv2 so với các hệ thống Object detection khác trên tập VOC 2007</figcaption>
</figure>
</div>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov2/comparision_voc2012.png" alt="Comparision on VOC 2012" class="figure-img">
  <figcaption class="figure-caption">So sánh mAP/FPS của YOLOv2 so với các hệ thống Object detection khác trên tập VOC 2012</figcaption>
</figure>
</div>
<hr>
<h3 id="ii-hiểu-paper">II. Hiểu Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Hiểu Paper&rdquo; này, mình sẽ đi vào phân tích cũng như dẫn lại các giải thích của tác giả về các lý do tại sao tác giả chọn hướng tiếp cận đó, cũng như các thuật ngữ, công thức trong paper&hellip;</p>
</blockquote>
<hr class="half-line">
<p>[<a href="#batch_normalization_b" style="color:red;">$ \triangleleft $</a>]<a name="batch_normalization"></a>:<strong>Batch normalization là gì?</strong><br>
Mỗi layer đều có input và output, output của layer này là input của layer tiếp sau đó.</p>
<p>Mỗi khi weight (parameters) được update trong khi training, input ở mỗi layer sẽ bị ảnh hưởng bởi tất cả các params ở các layer trước đó.</p>
<p>Vì vậy một sự thay đổi nhỏ của params sẽ khiến cho input của layer thay đổi càng lớn khi layer càng sâu hơn.</p>
<p>Sự thay đổi về distribution của input được gọi là &ldquo;<strong>Covariance Shift</strong>&rdquo;</p>
<p>Nếu sự thay đổi về distribution của input diễn ra ở các hidden layer, thì nó được gọi là &ldquo;<strong>Internal Covariance Shift</strong>&rdquo;</p>
<p>Sự thay đổi của distribution khiến cho các layers phải liên tục adapt với distribution mới của input. Điều này khiến cho quá trình training bị kém ổn định, khiến cho mất nhiều thời gian hơn để model hội tụ.</p>
<p>Batch normalization là kỹ thuật giúp giảm Internal covariance shift từ đó giúp quá trình training ổn định và cho phép sử dụng learning rate lớn hơn, nên nhanh hội tụ hơn. [<a href="#bn_paper" style="color:red;">?</a>]<a name="bn_paper_b"></a></p>
<hr class="half-line">
<p>[<a href="#anchor_box_b" style="color:red;">$ \triangleleft $</a>]<a name="anchor_box"></a>:<strong>Chi tiết về cách tính offset của Bounding box so với Anchor Boxes</strong></p>
<p>Gọi $t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$ lần lượt là các giá trị toạ độ tâm, rộng, dài và confidence score được predict ra từ network ứng với mỗi predicted Bounding box trên 1 grid cell. Đây là các giá trị được regression từ network nên có thể to nhỏ, âm, dương tuỳ ý.</p>
<p>Gọi $p_{w}, p_{h}$ là kích thước chiều rộng, dài của 1 Anchor box. Đây là giá trị kích thước theo pixel của Anchor box.</p>
<p>Gọi $C_{x}, C_{y}$ là offset từ góc top-left của input image. 2 giá trị này sẽ bằng gía trị index hàng và cột (i và j) của grid cell trên image.</p>
<p>Gọi $b_{x}, b_{y}, b_{w}, b_{h}, C$ lần lượt là toạ độ tâm, kích thước rộng, dài và confidence score của 1 predicted bounding box (trong số 5 Bounding box/ 1 grid cell). Các giá trị này sẽ được tính như sau:</p>
<p>\begin{multline*}
b_{x} = \sigma{t_{x}} + C_{x} \<br>
b_{y} = \sigma{t_{y}} + C_{y} \<br>
b_{w} = p_{w} * e^{t_{w}} \<br>
b_{h} = p_{h} * e^{t_{h}} \<br>
C = Pr(object) * IOU(b,object) = \sigma{t_{o}}
\end{multline*}</p>
<p>Giá trị $b_{x}, b_{y}$ sử dụng hàm **sigmoid**. $\sigma{t_{x}}, \sigma{t_{y}}$ sẽ nằm trong đoạn [0;1].</p>
<p>Giá trị C là giá trị IOU nên nằm trong đoạn [0;1]</p>
<p>Giá trị $b_{w}, b_{h} sử dụng luỹ thừa cơ số e (số Euler). $e^{t_{w}}, e^{t_{h}}$ sẽ nằm trong khoảng $[0; \infty]$, là giá trị tỉ lệ offset của bounding box so với anchor box</p>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov2/bounding_box_n_anchor_box.png" class="img-fluid" alt="Bounding box coordinates">
</div>
<hr class="half-line">
<p>[<a href="#k_means_b" style="color:red;">$ \triangleleft $</a>]<a name="k_means"></a>:<strong>Anchor boxes được tìm bằng K-means như thế nào?</strong></p>
<p><strong><em>Nhắc lại 1 chút về K-means:</em></strong></p>
<p>K-means là thuật toán phân cụm (clustering). Thuật toán sẽ nhóm các điểm dữ liệu có cùng một tính chất vào thành 1 nhóm. Số lượng chọn do mình chọn trước (hyperparams)</p>
<p>Ý tưởng của thuật toán K-means:</p>
<ul>
<li>Bước 1: Có X data point, giả sử ta chọn phân làm K cụm</li>
<li>Bước 2: <strong>Chọn ngẫu nhiên K data point</strong> trong X làm các <strong>centeroid</strong> của mỗi cụm</li>
<li>Bước 3: Tính toán <strong>khoảng cách Euclid</strong> (Euclidean distance) từ <strong>mỗi data point đến các centeroid</strong> này,  nếu khoảng cách đến center của cụm $K_{i}$ ngắn nhất thì phân data point đó về cụm $K_{i}$</li>
<li>Bước 4: Sau khi phân hết các data point về các cụm, tính toán lại centeroid ở mỗi cụm bằng cách lấy <strong>trung bình (mean)</strong> vị trí của các data point trong mỗi cụm đó</li>
<li>Bước 5: Lặp lại từ bước 2, nếu vị trí các center không thay đổi nữa thì dừng</li>
</ul>
<p>Tìm hiểu sâu hơn về K-means các bạn có thể đọc ở <a href="https://machinelearningcoban.com/2017/01/01/kmeans/#tom-tat-thuat-toan">trang Machine leanring cơ bản</a>)</p>
<p><strong><em>Phân cụm anchor boxes:</em></strong></p>
<ul>
<li>
<p>Mỗi data point sẽ chính là các ground truth box trong các image ở các tập VOC và COCO data set.</p>
</li>
<li>
<p>Chọn ngẫu nhiên K ground truth box trong dataset làm center của mỗi cụm</p>
</li>
<li>
<p>Khoảng cách từ mỗi box trong dataset đến center sẽ không được tính bằng Euclidean distance như thuật toán K-means gốc, mà sẽ tính bằng công thức sau:
\begin{equation*}
d(box,centroid) = 1 - IOU(box, centroid)
\end{equation}
Tác giả không lấy theo khoảng cách Euclid giữa các tâm của box vì nó sẽ không đánh giá đúng bản chất khoảng cách giữa các box (2 box lớn overlap nhau vẫn sẽ có euclid distance của tâm lớn hơn 2 box nhỏ không overlap).
Từ công thức ta có thể thấy giá trị khoảng cách $d$ sẽ nằm trong đoạn [0;1].</p>
</li>
<li>
<p>Sau khi phân hết data point về các cụm, centroid của mỗi cụm sẽ được tính bằng cách lấy trung bình (dài, rộng, tâm) của các box trong cụm đó</p>
</li>
<li>
<p>Tác giả thử với các lựa chọn K khác nhau và đi đến kết luận K = 5 sẽ là lựa chọn tối ưu nhất giữa model complexity và high recall</p>
</li>
</ul>
<div class="text-center my-3">
<img src="https://hungvm2.github.io/images/yolov2/k-anchor-boxes.png" class="img-fluid" alt="K Anchor Boxes">
</div>
<ul>
<li>Bức hình bên phải phía trên chỉ ra vị trí và kích thước centroid tương ứng của 5 cluster trên 2 tập COCO và VOC</li>
</ul>
<hr>
<h3 id="iii-nguồn-tham-khảo">III. Nguồn tham khảo:</h3>
<ul>
<li><a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a></li>
<li><a href="https://github.com/pjreddie/darknet">Darknet</a></li>
<li>[<a href="#bn_paper_b" style="color:red;">$ \triangleleft $</a>]<a name="bn_paper"></a>: <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization</a></li>
</ul>



                    </div>
                </div>
            </div>
        </div>
    </div>


    


    

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stan.min.js" defer></script>
        
        <script>
            window.addEventListener('load', function() {
                hljs.initHighlighting();
            }, true);
        </script>
    

    

    
    
        
<script src="https://hungvm2.github.io/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
</body>
</html>
