<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> Đọc hiểu Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1) - HungVM2&#39;s Notes </title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="referrer" content="no-referrer">
    <meta name="description" content="" />
    <meta property="og:site_name" content="HungVM2&#39;s Notes" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hungvm2.github.io/post/grasping-yolov1-content/" />
    <meta property="og:title" content="Đọc hiểu Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)" />
    <meta property="og:image" content="https://hungvm2.github.io" />
    <meta property="og:description" content="" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:title" content="Đọc hiểu Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://hungvm2.github.io" />

    <link rel="canonical" href="https://hungvm2.github.io/post/grasping-yolov1-content/">

    <link rel="stylesheet" href="https://hungvm2.github.io/css/bootstrap.min.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/github-gist.min.css" />
    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/custom.css" />

    

    <link rel="shortcut icon"
        href="https://hungvm2.github.io/images/favicon.png">

    
    <link href="https://hungvm2.github.io/index.xml" rel="alternate" type="application/rss+xml" title="HungVM2&#39;s Notes" />
    
</head>

<body>
    
    <div class="mt-xl header">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-auto">
                <a href="https://hungvm2.github.io">
                    <h1 class="name">HungVM2&#39;s notes</h1>
                </a>
            </div>
        </div>

        <div class="row justify-content-center">
            <ul class="nav nav-primary">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/">
                        
                        Articles
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/the-first-post">
                        
                        About
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/resume.pdf">
                        
                        Resume
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/contact">
                        
                        Contact
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

    <div class="content">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-sm-12 col-lg-8">
                    <h1 class="mx-0 mx-md-4">Đọc hiểu Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)</h1>
                    <div class="markdown">
                        
    <p>Trong bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper <a href="https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object">&ldquo;You Only Look One: Unified, Real-Time Object Detection&rdquo;</a> pulished năm 2016 nhé!
Bài viết sẽ chỉ tập trung vào phân tích những ý tưởng chính của YOLO, có lược bỏ một số phần không quá quan trọng</p>
<h4 id="phần-abstract">Phần Abstract</h4>
<p>Các nghiên cứu trước đây xử lý bài toán Object Detection theo hướng Classification (phân loại), còn tác giả YOLO lại tiếp cận theo hướng Regression (hồi quy).</p>
<p>YOLO là một Neural Network đơn nhất dự đoán đồng thời cả Bounding boxes và Class probabilities từ ảnh đầu vào qua một evaluation duy nhất.</p>
<p>Kiến trúc hợp nhất này giúp cho YOLO có thể xử lý ảnh ở tốc độ real-time 45fps. So sánh với các mô hình detect khác, YOLO sai nhiều hơn ở các lỗi về localization errors (sai vị trí của vật thể) nhưng lại ít sai hơn các lỗi false positive (nhận diện nhầm background là vật thể).</p>
<p>YOLO học được các đặc tính của objects một cách tổng quát hơn hẳn các phương pháp khác (như là DPM, R-CNN).</p>
<h4 id="phần-1-introduction">Phần 1: Introduction</h4>
<p>Các phương pháp trước đây detect object bằng cách phân loại object ở các vị trí và kích thước khác nhau trên ảnh đầu vào:</p>
<ul>
<li>Deformable Parts Models (DPM) sử dụng &ldquo;Sliding Window&rdquo; để Classify từng &ldquo;window&rdquo; trên toàn khung hình.</li>
<li>R-CNN sử dụng &ldquo;Region Proposal&rdquo; để đề xuất ra các Bounding boxes tiềm năng trong bức ảnh sau đó phân loại (Classify) trên những boxes này. Phân loại xong, một bước hậu xử lý được sử dụng để định vị lại chính xác các Bounding boxes, bỏ đi các boxes bị trùng lặp&hellip; Hướng tiếp cận này là một Pipeline phức tạp (gồm nhiều step/model nối tiếp nhau) nên rất chậm và khó để tối ưu.</li>
</ul>
<p>Thay vào đó, YOLO là một mạng đơn nhất chỉ cần nhìn ảnh đầu vào một lần duy nhất (You Only Look Once) để dự đoán đồng thời cả Bounding boxes và Class probabilities. Mạng YOLO này có các lợi ích sau:</p>
<ul>
<li>Cực kỳ nhanh, base model đạt tốc độ 45fps trên Titan X GPU nên có thể xử lý luồng video real-time</li>
<li>Dự đoán vật thể dựa trên ngữ cảnh toàn cục (global context) của bức ảnh, điều này giúp giảm các lỗi nhận diện nhầm backgound là vật thể (false positive)</li>
<li>Khả năng tổng quát hoá cao hơn. Khi train trên dữ liệu tự nhiên và test trên dữ liệu artwork, YOLO đạt độ chính xác cao hơn các phương pháp khác như DPM, R-CNN</li>
</ul>
<p>Tuy nhiên YOLO vẫn có nhược điểm: Độ chính xác thấp hơn các phương pháp state-of-the-art khác, khó khăn khi xác định chính xác vị trí của các vật thể, đặc biệt các vật thể cỡ nhõ.</p>
<h4 id="phần-2-unified-detection-mô-hình-phát-hiện-hợp-nhất">Phần 2: Unified Detection (Mô hình Phát hiện hợp nhất)</h4>
<blockquote>
<p>Phần này bắt đầu đi vào chi tiết kiến trúc của YOLO, sau khi mô tả xong các thành phần trong kiến trúc thì mình sẽ phân tích chi tiết lí do tại sao tác giả lại sử dụng các thành phần đó nhé!</p>
</blockquote>
<p>Hệ thống YOLO chia ảnh đầu vào thành một lưới SxS các ô vuông (grid cell). Nếu trung tâm (center) của vật thể rơi vào ô vuông nào, thì ô vuông đó sẽ chịu trách nhiệm phát hiện vật thể đó. Tác giả lấy S = 7</p>
<p>Mỗi grid cell cần dự đoán: B bounding boxes, confidence scores của B bounding boxes và C scores của các lớp object, trong đó:</p>
<ul>
<li>B: Số lượng Bounding boxes cần dự đoán trên 1 cell. Tác giả chọn B = 2</li>
<li>Mỗi bounding box gồm 4 gía trị toạ độ (coordinates): x, y, w, h. Trong đó (x,y) là toạ độ tâm của box được dự đoán, (w,h) là chiều dài và rộng của box</li>
<li>Confidence score (CS): mỗi bounding box có 1 confidence score tương ứng, vậy là sẽ có B confidence scores, tính theo công thức</li>
</ul>
<div class="text-center"><img src="https://hungvm2.github.io/images/yolov1/C_formular.png" alt="Confidence Score"></div>
    Nếu grid cell không có object nào Pr(Object) = 0. Nếu grid cell có object thì Pr(Object) = 1 => CS = IOU của ground truth box và predicted box
+ C: giá trị xác suất nhân diện của C lớp đối tượng trong model. . Tác giả lấy C = 20 (số lớp đối tượng trong tập dữ liệu PASCAL VOC)
<div class="text-center">C<sub>i</sub> = Pr(Class<sub>i</sub>)|Object)</div>
<p>=&gt; Kích thước (shape) của final output sẽ là:</p>
<div class="text-center">(S , S , B * 4 + B + C) = (7 , 7 , 2 * 4 + 2 + 20) = (7 , 7 , 30)</div>
<h5 id="phần-21-network-design">Phần 2.1. Network Design</h5>
<p>Kiến trúc mạng được truyền cảm hứng bởi mạng GoogLeNet (Inception) (Sử dụng 1x1 Conv layer)</p>
<p>Mạng gồm 24 Convolutional Layers để trích xuất đặc trưng (extract features) và 2 Fully Connected Layers để dự đoán final output. Kiến trúc như bên dưới</p>
<div class="text-center">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/network_architecture.png" alt="Network Architecture" class="figure-img">
  <figcaption class="figure-caption">Ảnh 1: Kiến trúc mạng detection của YOLO</figcaption>
</figure>
</div>
<h5 id="phần-22-training">Phần 2.2. Training</h5>
<ol>
<li>Bước 1: đặt 20 lớp Conv Layers đầu tiên của mạng detection (Ảnh 1) trong 1 kiến trúc mạng Classification (Input 224x224 =&gt; 24 Conv layers =&gt; 1 Average Pooling layer =&gt; 1 Fully Connected Layer), train trên tập ImageNet 1000 classes. Bước này giúp 24 lớp Conv Layers có bộ weights tốt trong việc trích xuất đặc trưng từ ảnh.</li>
<li>Bước 2: Sử dụng transfer learning, đặt weights của 20 lớp Conv Layers đã được train ở bước 1 vào mạng detection (Ảnh 1). Do Weights của Conv Layers không phụ thuộc vào Input size nên tăng Input size lên 448x448 để mạng học được những đặc trưng chi tiết hơn của ảnh.</li>
<li>Bước 3: Lấy kết quả output của mạng detection, đem xử lý lại trước khi đặt vào hàm loss</li>
</ol>



                    </div>
                </div>
            </div>
        </div>
    </div>

    <section id="comments">
    <div class="py-3 content">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-sm-12 col-lg-8">
                    <div class="comments">
                        <script src="https://utteranc.es/client.js" repo=""
                            issue-term="pathname" label="comment" theme="github-light" crossorigin="anonymous" async>
                            </script>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
    


    

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stan.min.js" defer></script>
        
        <script>
            window.addEventListener('load', function() {
                hljs.initHighlighting();
            }, true);
        </script>
    

    

    
    
        

    
</body>

</html>
