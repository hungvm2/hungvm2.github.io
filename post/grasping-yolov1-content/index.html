<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title> Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1) - HungVM2&#39;s Notes </title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="referrer" content="no-referrer">
    <meta name="description" content="" />
    <meta property="og:site_name" content="HungVM2&#39;s Notes" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://hungvm2.github.io/post/grasping-yolov1-content/" />
    <meta property="og:title" content="Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)" />
    <meta property="og:image" content="https://hungvm2.github.io" />
    <meta property="og:description" content="" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:title" content="Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)" />
    <meta name="twitter:description" content="" />
    <meta name="twitter:image" content="https://hungvm2.github.io" />

    <link rel="canonical" href="https://hungvm2.github.io/post/grasping-yolov1-content/">

    <link rel="stylesheet" href="https://hungvm2.github.io/css/bootstrap.min.css" />

    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/github-gist.min.css" />
    
    <link rel="stylesheet" href="https://hungvm2.github.io/css/custom.css" />

    

    <link rel="shortcut icon"
        href="https://hungvm2.github.io/images/favicon.png">

    
    <link href="https://hungvm2.github.io/index.xml" rel="alternate" type="application/rss+xml" title="HungVM2&#39;s Notes" />
    

</head>

<body>
    
    <div class="mt-xl header">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-auto">
                <a href="https://hungvm2.github.io">
                    <h1 class="name">HungVM2&#39;s notes</h1>
                </a>
            </div>
        </div>

        <div class="row justify-content-center">
            <ul class="nav nav-primary">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/">
                        
                        Articles
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/the-first-post">
                        
                        About
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/resume.pdf">
                        
                        Resume
                    </a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="https://hungvm2.github.io/contact">
                        
                        Contact
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

    <div class="content">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-sm-12 col-lg-8">
                    <h1 class="mx-0 mx-md-4">Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)</h1>
                    <div class="markdown">
                        
    <p>Trong bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper <a href="https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object">&ldquo;You Only Look One: Unified, Real-Time Object Detection&rdquo;</a> pulished năm 2016 nhé!</p>
<h3 id="i-đọc-paper">I. Đọc Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Đọc Paper&rdquo; này, mình sẽ đi thứ tự theo các section của paper và tóm lược lại những ý chính của tác giả, có bỏ qua một số phần không quá quan trọng. Khi đọc sẽ có các reference (có dạng [number]) cuối một câu, hoặc cụm từ. Bạn có thể click vào các link đó để dẫn xuống phần &ldquo;Hiểu Paper&rdquo; bên dưới để đọc sâu hơn về vấn đề được đề cập.</p>
</blockquote>
<h4 id="1-giới-thiệu-chung-về-mô-hình-model">1: Giới thiệu chung về mô hình (model)</h4>
<p>Các nghiên cứu trước đây xử lý bài toán Object Detection theo hướng Classification (phân loại) [<a href="#previous_model" style="color:red;">1</a>]<a name="previous_model_b" style="color:red;"></a>, còn tác giả YOLO lại <strong>tiếp cận theo hướng Regression</strong> (hồi quy).</p>
<p>YOLO là <strong>một Neural Network đơn nhất</strong> dự đoán đồng thời cả Bounding boxes và Class probabilities từ ảnh đầu vào qua một evaluation duy nhất.</p>
<p>Kiến trúc hợp nhất này giúp cho YOLO có các ưu điểm:</p>
<ul>
<li>Có thể xử lý ảnh ở <strong>tốc độ real-time</strong> $45$FPS. So sánh với các mô hình detect khác, YOLO mắc lỗi nhiều hơn về Localization Errors (sai vị trí của vật thể) nhưng lại ít hơn các lỗi False Positive (nhận diện nhầm Background là vật thể).</li>
<li>Học được các đặc tính của objects một cách <strong>tổng quát hơn</strong> hẳn các phương pháp khác (như là DPM, R-CNN).</li>
</ul>
<p>Tuy nhiên YOLO vẫn có nhược điểm:</p>
<ul>
<li>Độ chính xác thấp hơn các phương pháp state-of-the-art khác</li>
<li>Khó khăn khi xác định chính xác vị trí của các vật thể (Localization), đặc biệt các vật thể cỡ nhỏ.</li>
</ul>
<h4 id="2-kiến-trúc-của-yolo">2: Kiến trúc của YOLO</h4>
<p>Hệ thống YOLO chia ảnh đầu vào thành một lưới SxS các ô vuông (grid cell). Nếu trung tâm (center) của vật thể rơi vào ô vuông nào, thì ô vuông đó sẽ chịu trách nhiệm phát hiện vật thể đó [<a href="#r1" style="color:red;">1</a>].</p>
<p>Mỗi grid cell cần dự đoán: $B$ bounding boxes, $B$ confidence scores của B bounding boxes và $C$ giá trị xác suất của các lớp đối tượng (các class trong dataset), trong đó:</p>
<ul>
<li>Mỗi bounding box gồm 4 giá trị toạ độ (coordinates): $(x, y, w, h)$. Trong đó $(x,y)$ là toạ độ tâm của box được dự đoán, $(w,h)$ là chiều dài và rộng của box</li>
<li>Mỗi bounding box có 1 confidence score $S_{z}$ tương ứng, tính theo công thức:</li>
</ul>
<p>\begin{equation}S_{z} = Pr(Object) * IOU_{pred_{z}}^{truth}, \quad z = (1,&hellip;,B)\end{equation}</p>
<p>Nếu grid cell không có object nào $Pr(Object) = 0$. Nếu grid cell có object thì $Pr(Object) = 1$</p>
<ul>
<li>Giá trị xác suất từng class trong một grid cell được tính theo công thức:</li>
</ul>
<p>\begin{equation}Pr(C_{k}) = Pr(Class_{k}|Object), \quad  k = (1,2&hellip;,C)\end{equation}</p>
<p>Kích thước của Output Layer:</p>
<p>\begin{equation}Ouput\quad Shape = (S , S , B * 4 + B + C) = (S, S , 5 * B + C)\end{equation}</p>
<h5 id="21-kiến-trúc-mạng">2.1. Kiến trúc mạng</h5>
<p>Kiến trúc mạng được truyền cảm hứng bởi mạng GoogLeNet (Inception) [<a href="#1by1_conv" style="color:red;">1</a>]<a name="1by1_conv_b" style="color:red;"></a></p>
<p>Mạng gồm 24 Convolutional Layers để trích xuất đặc trưng (extract features) và 2 Fully Connected Layers để dự đoán final output [<a href="#network_detail" style="color:red;">1</a>]. Kiến trúc như bên dưới:</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/network_architecture.png" alt="Network Architecture" class="figure-img">
  <figcaption class="figure-caption">Ảnh 1: Kiến trúc mạng detection của YOLO</figcaption>
</figure>
</div>
<h5 id="22-các-bước-khi-huấn-luyện-mô-hình-training">2.2. Các bước khi huấn luyện mô hình (training):</h5>
<p><strong>Bước 1</strong>: đặt 20 lớp Conv Layers đầu tiên của mạng detection (Ảnh 1) trong 1 kiến trúc mạng Classification (Input 224x224 =&gt; 24 Conv layers =&gt; 1 Average Pooling layer =&gt; 1 Fully Connected Layer), train trên tập ImageNet 1000 classes. Bước này giúp 24 lớp Conv Layers có bộ weights tốt trong việc trích xuất đặc trưng từ ảnh.
<strong>Bước 2</strong>: Sử dụng Transfer Learning, đặt weights của 20 lớp Conv Layers đã được train ở bước 1 vào mạng detection (Ảnh 1). Do Weights của Conv Layers không phụ thuộc vào Input size nên tăng Input size lên 448x448 để mạng học được những đặc trưng chi tiết hơn của ảnh.
<strong>Bước 3</strong>: Lấy kết quả output của mạng detection, biến đổi lại các giá trị nằm trong khoảng $[0,1]$ trước khi đặt vào hàm loss [<a href="#feature_scaling" style="color:red;">1</a>]</p>
<ul>
<li>Normalize (w,h) của bounding box để w,h trong đoạn [0,1]: (w = w/W , h = h/H | W,H: size của input)</li>
<li>Biến đổi toạ độ (x,y) thành toạ độ offset từ top-left của grid cell mà nó nằm trong để x, y nằm trong đoạn [0,1]. VD: x,y nằm chính giữa cell thì x = 0.5, y = 0.5</li>
<li>Confidence score của bounding box = IOU nên cũng nằm trong đoạn [0,1]</li>
<li>Ci là xác suất detect ra vật thể class i nên cũng nằm trong đoạn [0,1]</li>
</ul>
<p><strong>Bước 4</strong>: Xây dựng hàm Loss [<a href="#sse" style="color:red;">1</a>]</p>
<p>Công thức tổng quát:</p>
<p>
\begin{equation}
\begin{split}
\lambda_{coord} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^{2}] \\
\end{split}
\begin{split}
+ \lambda_{coord}\displaystyle\sum_{i=0}^{S^{2}}\displaystyle\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}[(\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}] \\
\end{split}
\begin{split} 
+ \displaystyle\sum_{i=0)^{S^{2}} \mathbb{1}_{i}^{obj} \displaystyle\sum_{c \in classes}(p_{i}(c) - \hat{p_{i}}(c))^{2}
\end{split} 
\end{equation}
</p>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/loss_function.png" alt="Loss Function" class="figure-img">
  <figcaption class="figure-caption">Ảnh 2: Hàm loss</figcaption>
</figure>
</div>
<p>Để đánh giá YOLO trên tập PASCAL POC, tác giả lấy S = 7, B = 2 [<a href="#r2" style="color:red;">2</a>], C = 20 (số lớp đối tượng trong tập dữ liệu PASCAL VOC), nên shape của output tensor sẽ là:</p>
<p>\begin{equation}Ouput \quadShape = (S , S , B * 5 + C) = (7 , 7 , 2 * 5 + 20) = (7 , 7 , 30)\end{equation}</p>
<p>Công thức trở về dạng:</p>
<p>Trong đó:</p>
<div>
- $λ_{coord} = 5$ , $λ_{noobj} = 0.5$ <br> 
- $\mathbb{1}_{i}^{obj}$ nhận giá trị 1 nếu object xuất hiện ở cell thứ $i$, 0 nếu ngược lại và $\mathbb{1}_{ij}^{obj}$ nhận giá trị 1 nếu bounding box thứ $j$ là "predictor", 0 nếu ngược lại
</div>
<p><strong>Bước 5</strong>: Cấu hình và training</p>
<ul>
<li>Số Epochs: 135</li>
<li>Dataset: PASCAL VOC 2017 và 2012</li>
<li>Batch size: 64 (nếu có $m$ data points trong tập train thì mỗi epoch sẽ có $m/64$ iterations)</li>
<li>Momentum: 0.9 (Gradient Descent có sử dụng kỹ thuật momentum để hội tụ nhanh hơn)</li>
<li>Decay: 0.0005 (ý của tác giả ở đây có lẽ là Weight decay, một kỹ thuật regularization giúp giảm High Variance (Overfitting) (hơi giống L2 Regularization))</li>
<li>Learning rate (LR) schedule: tăng dần LR từ $10^{-3}$ tới $10^{-2}$ thay vì để LR cao từ đầu để tránh model bị phân kỳ (diverge) giai đoạn đầu, sau đó giữ nguyên LR $10^{-2}$ trong 75 epochs, giảm xuống $10^{-3}$ cho 30 epochs tiếp theo, giảm xuống $10^{-4}$ cho 30 epochs cuối.</li>
<li>Data augmentation (làm giàu bộ dữ liệu): Random scaling (phóng to, thu nhỏ ngẫu nhiên), random translation (xê dịch vị trí pixel ngẫu nhiên) tới 20% kích thước ảnh. Ngẫu nhiên điều chỉnh độ phơi sáng (exposure) và bão hoà (saturation) của ảnh tới 1.5 lần trong không gian màu HSV.</li>
</ul>
<h5 id="23-inference">2.3. Inference</h5>
<p>Thường tâm của object sẽ chỉ nằm trong 1 gird cell, tuy nhiên có những trường hợp như object lớn hoặc object nằm gần viền của nhiều cell nên dễ bị detect ra nhiều box cho cùng một object.
Thêm bước Non-max suppression (NMS) giúp triệt tiêu những box bị trùng khi detect cùng một object. Cải thiện thêm 2-3% mAP cho model.</p>
<p>Pipe line của prediction:</p>
<blockquote>
<p>Image (448,448,3) =&gt; Network Prediction =&gt; Output (7,7,30) =&gt; Tính lại class probabilities =&gt; Bỏ qua các box có low probabilities =&gt; Non-max suppression =&gt; Final prediction</p>
</blockquote>
<p>Xác suất nhận diện của các class trong ảnh sẽ được tính theo công thức sau:</p>
<div class="text-center my-3">
  <img src="https://hungvm2.github.io/images/yolov1/final_class_probs.png" alt="Class Probabilities" class="figure-img">
</div>
<h4 id="3-so-sánh-độ-chính-xác-và-hiệu-năng-với-các-mô-hình-khác">3. So sánh độ chính xác và hiệu năng với các mô hình khác</h4>
<ul>
<li>So sánh tốc độ/mAP của YOLO, biến thể Fast YOLO với các mô hình khác</li>
</ul>
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/fps_map_comparison.png" alt="FPS/mAP Comparison" class="figure-img">
  <figcaption class="figure-caption">Ảnh 3: So sánh trên tập PASCAL VOC 2007</figcaption>
</figure>
</div>
+ Phân tích các lỗi của YOLO so với Fast R-CNN trên tập VOC 2007
<div class="text-center my-3">
<figure class="figure">
  <img src="https://hungvm2.github.io/images/yolov1/error_analysis.png" alt="Error Analysis" class="figure-img">
  <figcaption class="figure-caption">Ảnh 4: So sánh lỗi của YOLO và Fast R-CNN</figcaption>
</figure>
</div>
<h3 id="ii-hiểu-paper">II. Hiểu Paper</h3>
<blockquote>
<p>Trong phần &ldquo;Hiểu Paper&rdquo; này, mình sẽ đi vào phân tích các lý do tại sao tác giả chọn hướng tiếp cận đó, cũng như giải thích và làm rõ thêm các thuật ngữ, công thức&hellip;</p>
</blockquote>
<p><a name="previous_model" style="color:red;"></a>[<a href="#previous_model_b" style="color:red;">1</a>]: Kỹ thuật sử dụng ở một số phương pháp trước đây:</p>
<ul>
<li>Deformable Parts Models (DPM) sử dụng &ldquo;Sliding Window&rdquo; sau đó phân loại trên các &ldquo;Window&rdquo; này</li>
<li>R-CNN sử dụng &ldquo;Region Proposal&rdquo; để đề xuất ra các Bounding boxes tiềm năng, sau đó phân loại (Classify) trên những boxes này và một bước hậu xử lý được sử dụng để định vị lại chính xác các Bounding boxes, bỏ đi các boxes bị trùng lặp&hellip; Hướng tiếp cận này là một Pipeline phức tạp (gồm nhiều step/model nối tiếp nhau) nên rất chậm và khó để tối ưu.</li>
</ul>
<p>[<a name="network_detail" style="color:red;">1</a>]: Chi tiết về kiến trúc của network, bạn có thể xem trong file config của YOLO, thứ tự các layer trong file config chính là thứ tự các layer của YOLO Network <a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov1.cfg">https://github.com/pjreddie/darknet/blob/master/cfg/yolov1.cfg</a></p>
<p>[<a name="r1" style="color:red;">1</a>]: Grid cell được chia ở bước nào?</p>
<p>[<a href="#1by1_conv_b" style="color:red;">1</a>]<a name="1by1_conv" style="color:red;"></a>: (Sử dụng 1x1 Conv layer)</p>
<p>[<a name="feature_scaling" style="color:red;">1</a>]: Tại sao phải có bước xử lý này? Đây là bước Scaling Features: đưa khoảng giá trị của các feature về cùng một scale trước khi vào hàm loss để giúp mô hình hội tụ nhanh và ổn định hơn khi training</p>
<p>[<a name="sse" style="color:red;">1</a>]: Hàm loss sử dụng là hàm Sum Squared Error. Tác giả giải thích lí do sử dụng hàm này vì nó dễ để tối ưu.</p>
<p>Tuy nhiên hàm này không đạt được độ chính xác AP (Average Precision) như mong đợi vì:</p>
<ul>
<li>Hàm này coi các giá trị về lỗi toạ độ (localization error) và lỗi phân loại (classification error) là tương đương với nhau (có thể thấy rõ trên công thức ở Ảnh 2)</li>
<li>Những grid cell không chứa object thì các giá trị Confidence scores sẽ bằng 0, việc này làm tăng sức mạnh của gradient (overpowering the gradient) ở những cell có chứa objects, khiến model bị mất ổn định. Để giải quyết, tác giả thêm các hệ số &ldquo;$λ_{coord} = 5$&rdquo; và &ldquo;$λ_{noobj} = 0.5$&rdquo; để tăng loss từ toạ độ bounding box và giảm loss từ Confidence score.</li>
</ul>
<p>Còn một vấn đề: box lớn thì sai số về kích thước (w, h) giữa predicted value và ground truth value cũng sẽ lớn hơn là box nhỏ. Để xử lý vấn đề này, tác giả sử dụng sai số giữa căn bậc hai của kích thước predict và ground truth thay vì giá trị w,h gốc [<a href="#square_root_wh" style="color:red;">1</a>].</p>
<p>Khi training, mỗi grid cell cần predict B bounding boxes, tuy nhiên sẽ chỉ Bounding box có IOU lớn nhất so với ground truth box mới được chọn lựa để tính toán hàm loss (chọn làm &ldquo;predictor&rdquo;). Điều này sẽ giúp mỗi predicted Bounding box sẽ chỉ chuyên môn hoá vào việc detect object có kích thước, tỉ lệ nhất định, giúp tăng Recall của model</p>
<p>[<a name="square_root_wh" style="color:red;">1</a>]: Ví dụ: Box To có predicted W = 9 và ground truth W = 8 =&gt; độ lệch = 1, Box Nhỏ có predicted W = 3 và ground truth W = 2 =&gt; độ lệch = 1. Rõ ràng Box nhỏ có sai số lớn hơn xét theo tỉ lệ, nhưng số tuyệt đối độ lệch thì như nhau</p>
<p>[<a name="r2" style="color:red;">2</a>]: Tại sao lại chọn $B=2$?</p>
<h3 id="iii-nguồn-tham-khảo">III. Nguồn tham khảo:</h3>



                    </div>
                </div>
            </div>
        </div>
    </div>

    <section id="comments">
    <div class="py-3 content">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-sm-12 col-lg-8">
                    <div class="comments">
                        <script src="https://utteranc.es/client.js" repo=""
                            issue-term="pathname" label="comment" theme="github-light" crossorigin="anonymous" async>
                            </script>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
    


    

    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" defer></script>
        
            <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/stan.min.js" defer></script>
        
        <script>
            window.addEventListener('load', function() {
                hljs.initHighlighting();
            }, true);
        </script>
    

    

    
    
        
<script src="https://hungvm2.github.io/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
  MathJax = {
    tex: {
      tags: 'all',
    inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
