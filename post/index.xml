<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on HungVM2&#39;s Notes</title>
    <link>https://hungvm2.github.io/post/</link>
    <description>Recent content in Posts on HungVM2&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 21 Jun 2021 22:44:24 +0700</lastBuildDate><atom:link href="https://hungvm2.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Đọc và phân tích Paper &#34;You Only Look One: Unified, Real-Time Object Detection&#34;(YOLOv1)</title>
      <link>https://hungvm2.github.io/post/grasping-yolov1-content/</link>
      <pubDate>Mon, 21 Jun 2021 22:44:24 +0700</pubDate>
      
      <guid>https://hungvm2.github.io/post/grasping-yolov1-content/</guid>
      <description>\begin{multiline} \lambda_{coord} \displaystyle \sum_{i=0}^{S^{2}} \displaystyle \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^{2}] \\ + \lambda_{coord}\displaystyle\sum_{i=0}^{S^{2}}\displaystyle\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}[(\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}] \\ + \displaystyle \sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{obj} \displaystyle \sum_{c \in classes}(p_{i}(c) - \hat{p_{i}}(c))^{2} \end{multiline}  Trong bài viết này, chúng ta hãy cùng đọc và phân tích nội dung của paper [&#34;You Only Look One: Unified, Real-Time Object Detection&#34;](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object) pulished năm 2016 nhé! I. Đọc Paper  Trong phần &amp;ldquo;Đọc Paper&amp;rdquo; này, mình sẽ đi thứ tự theo các section của paper và tóm lược lại những ý chính của tác giả, có bỏ qua một số phần không quá quan trọng.</description>
    </item>
    
    <item>
      <title>Yolo Versions Comparison</title>
      <link>https://hungvm2.github.io/post/yolo-versions-comparison/</link>
      <pubDate>Sun, 13 Jun 2021 16:53:39 +0700</pubDate>
      
      <guid>https://hungvm2.github.io/post/yolo-versions-comparison/</guid>
      <description>Yolov4 Scaled:
 Input: 512x512x3 Scaling Cross-Stage-Partial (CSP) Network approach  Yolov4:
 Backbone: CSPDarknet53  Data Augmentation: CutMix and Mosaic Mish Activation Cross-stage partial connections (CSP) Multiinput weighted residual connections (MiWRC) DropBlock regularization Class label smoothing   Detector:  Neck: PANet path-aggregation Head: same as YOLOv3 CIoU-loss CmBN DropBlock regularization Mosaic data augmentation Self-Adversarial Training Eliminate grid sensitivity Using multiple anchors for a single ground truth Cosine annealing scheduler Optimal hyperparameters Random training shapes Mish activation, SPP-block SAM-block DIoU-NMS    Yolov3:</description>
    </item>
    
  </channel>
</rss>
